<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2020/07/11/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Bregman_Divergence_and_Mirror_Descent</title>
    <url>/2020/07/11/Bregman-Divergence-and-Mirror-Descent/</url>
    <content><![CDATA[<h1 id="Bregman-Divergence-and-Mirror-Descent"><a href="#Bregman-Divergence-and-Mirror-Descent" class="headerlink" title="Bregman Divergence and Mirror Descent"></a>Bregman Divergence and Mirror Descent</h1><blockquote>
<p>[1] <a href="http://users.cecs.anu.edu.au/~xzhang/teaching/bregman.pdf" target="_blank" rel="noopener">Bregman Divergence and Mirror Descent</a></p>
</blockquote>
<hr>
<h2 id="I-Bregman-Divergence"><a href="#I-Bregman-Divergence" class="headerlink" title="I. Bregman Divergence"></a>I. Bregman Divergence</h2><p><strong>动机</strong></p>
<ul>
<li>将欧几里得距离的平方概括为一类距离，这些距离都具有相似的性质。</li>
<li>在机器学习、聚类、指数族等方面有很多应用。</li>
</ul>
<p><strong>定义1（Bregman divergence）</strong> 函数$\psi : \Omega \rightarrow \reals$满足：<br>a). 严凸<br>b). 连续可微<br>c). 定义在一个封闭的凸集$\Omega$上。<br>那么<code>Bregman散度</code>可以定义为：<br>$$<br>\text{Div}_{\psi} (x, y) = \psi(x) - \psi(y) - \left&lt; \nabla \psi (y), x - y \right&gt;.<br>\tag{1}<br>$$<br>即为$\psi$在$x$处的值与$\psi$在$y$周围的一阶泰勒展开式取$x$的值之间的差。</p>
<table>
<thead>
<tr>
<th>函数名</th>
<th>$\psi (x)$</th>
<th>函数定义域</th>
<th>$\text{Div}_{\psi} (x, y)$</th>
</tr>
</thead>
<tbody><tr>
<td>Squared norm</td>
<td>$\frac{1}{2}x^2$</td>
<td>$(-\infty, +\infty)$</td>
<td>$\frac{1}{2}(x-y)^2$</td>
</tr>
<tr>
<td>Shannon entropy</td>
<td>$x \log x - x$</td>
<td>$[0, +\infty)$</td>
<td>$x \log \frac{x}{y} - x +y$</td>
</tr>
<tr>
<td>Bit entropy</td>
<td>$x \log x  + (1 - x) \log (1 - x)$</td>
<td>$[0, 1]$</td>
<td>$x \log \frac{x}{y} + (1 - x) \log \frac{(1 - x)}{(1 - y)}$</td>
</tr>
<tr>
<td>Burg entropy</td>
<td>$- \log x$</td>
<td>$(0, +\infty)$</td>
<td>$\frac{x}{y} - \log \frac{x}{y} - 1$</td>
</tr>
<tr>
<td>Hellinger</td>
<td>$- \sqrt{1 - x^2}$</td>
<td>$[-1, 1]$</td>
<td>$(1 - xy)(1 - y^2)^{-\frac{1}{2}} - (1 - x^2 )^{\frac{1}{2}}$</td>
</tr>
<tr>
<td>$l_p$ quasi-norm</td>
<td>$- x^p \quad (0&lt;p&lt;1)$</td>
<td>$[0, +\infty)$</td>
<td>$-x^p+pxy^{p-1}-(p-1)y^p$</td>
</tr>
<tr>
<td>$l_p$ norm</td>
<td>$- \vert x \vert^p \quad (1&lt;p&lt;\infty)$</td>
<td>$(-\infty, +\infty)$</td>
<td>$\vert x \vert^p - p x y^{p-1} \text{sgn}(y) + (p-1) \vert y \vert^p$</td>
</tr>
<tr>
<td>Exponential</td>
<td>$\exp(x)$</td>
<td>$(-\infty, +\infty)$</td>
<td>$\exp(x) - \left(x - y + 1 \right) \exp(y)$</td>
</tr>
<tr>
<td>Hellinger</td>
<td>$\frac{1}{x}$</td>
<td>$(0, +\infty)$</td>
<td>$\frac{1}{x} + \frac{x}{y^2} - \frac{2}{y}$</td>
</tr>
</tbody></table>
<p><strong>例子</strong></p>
<ul>
<li>欧式距离。令$\psi(x)=\frac{1}{2}|x|^2$，则$\text{Div}_{\psi}(x,y) = \frac{1}{2} | x - y |^2$。</li>
<li>$\Omega = { x \in \reals_{+}^{n} : \sum_{i} x_i = 1 }$，且$\psi(x) = \sum_{i} x_i \log x_i$。那么$\text{Div}<em>{\psi}(x,y) = \sum</em>{i} x_i \log \frac{x_i}{y_i}, x, y \in \Omega$。这就是所谓的相对熵，或概率分布$x$和$y$之间的Kullback-Leibler散度。</li>
<li>$l_p$范式。令$p \geq 1$且$\frac{1}{p} + \frac{1}{q} = 1$，$\psi(x)=\frac{1}{2}|x|<em>q^2$。那么$\text{Div}</em>{\psi}(x,y) = \frac{1}{2}|x|_q^2 + \frac{1}{2}|y|_q^2 - \left&lt; x , \nabla \frac{1}{2}|y|_q^2 \right&gt;$。注意$\frac{1}{2}|y|_q^2$不一定是可连续微分的，这就使得这种情况与我们的定义不完全一致。</li>
</ul>
<p><strong>Properties of Bregman divergence</strong></p>
<p><code>1.</code> Bregman散度$\text{Div}_{\psi}(x,y)$关于第一个变量$x$是严格凸函数，那么函数$\psi$的严格凸性并不重要。</p>
<blockquote>
<p><em>原文：</em> Strict convexity in the first argument $x$. Trivial by the strict convexity of $\psi$.</p>
</blockquote>
<p><code>2.</code> 非负性。$\forall x, y$，有$\text{Div}_{\psi}(x,y) \geq 0$，等号当且仅当$x=y$时取得。不要求严格凸亦成立（Trivial by strict convexity.）。</p>
<p><code>3.</code> 非对称性。一般情况下$\text{Div}<em>{\psi}(x,y) \neq \text{Div}</em>{\psi}(y,x)$，例如KL散度。对称性并不是一直有用。</p>
<p><code>4.</code> 关于第二变量$y$非凸。令$\Omega = [1, \infty), \psi(x) = - \log x$。那么$\text{Div}_{\psi}(x,y) = - \log x + \log y + \frac{x-y}{y}$。计算其关于$y$的二阶偏导为$\frac{1}{y^2}(\frac{2x}{y}-1)$，当$2x &lt; y$时为负。</p>
<p><code>5.</code> 关于$\psi$的线性。对于任意$a &gt; 0$，有$\text{Div}<em>{\psi + a \phi}(x,y)= \text{Div}</em>{\psi}(x,y) + a \text{Div}_{\phi}(x,y)$。</p>
<p><code>6.</code> 关于$x$的梯度。$\frac{\partial }{ \partial x} \text{Div}_{\psi}(x,y) = \nabla \psi (x) - \nabla \psi (y)$。关于$y$的梯度比较棘手，不常用。</p>
<p><code>7.</code> 广义三角形不等式：<br>$$<br>\begin{aligned}<br>    \text{Div}<em>{\psi}(x,y) + \text{Div}</em>{\psi}(y,z) &amp;= \psi(x) - \psi(y) - \left&lt; \nabla \psi (y), x - y \right&gt; + \psi(y) - \psi(z) - \left&lt; \nabla \psi (z), y - z \right&gt; \<br>    &amp;= \text{Div}_{\psi}(x,z) + \left&lt; x - y, \nabla \psi (z) - \nabla \psi (y) \right&gt;.<br>\end{aligned}<br>\tag{2}<br>$$</p>
<p><code>8.</code> 特例，如果$\psi$对于某些范式和模数$\sigma$满足：<br>$$<br>\psi (x) \geq \psi(y) + \left&lt; \nabla \psi(y), x - y\right&gt; + \frac{\sigma}{2} | x - y |^2.<br>\tag{3}<br>$$<br>则称$\psi$是$\sigma$-<code>强凸函数</code>。需要注意的是，并不需要是欧几里得范式。如果是欧式范式，那么上式条件等价于$\psi(x) - \frac{\sigma}{2} | x |^2$是凸函数。比如，KL散度中的$\psi (x) = \sum_{i} x_i$是在$\Omega = { x \in \reals_{+}^{n} : \sum_{i} x_i = 1 }$上$l_1$范式下的1-强凸函数。当$\psi$是$\sigma$-强凸函数，有：<br>$$<br>\text{Div}_{\psi}(x,y) \geq \frac{\sigma}{2} | x - y |^2.<br>\tag{4}<br>$$</p>
<p><em>证明：</em> 根据定义有$\text{Div}_{\psi} (x, y) = \psi(x) - \psi(y) - \left&lt; \nabla \psi (y), x - y \right&gt; \geq \frac{\sigma}{2} | x - y |^2$。</p>
<p><code>9.</code> 对偶性。假设$\psi$是强凸函数，那么<br>$$<br>(\nabla \psi^{<em>}) \nabla \psi (x) = x, \qquad \text{Div}<em>{\psi} (x, y) = \text{Div}</em>{\psi^{</em>}} (\nabla \psi (y), \nabla \psi (x)).<br>\tag{5}<br>$$<br><em>证明：</em> 先证第一个等式。根据：<br>$$<br>\psi^{<em>}(y) = \sup_{z \in \Omega} { \left&lt; z, y \right&gt; - \psi (z) }.<br>\tag{6}<br>$$<br>$\sup$是可以实现的，因为$\psi$是强凸的，且定义域$\Omega$是闭集。如果在$x$处取得最大值，当且仅当$y = \nabla \psi (x)$（令$\frac{\partial}{\partial z} \left( \left&lt; z, y \right&gt; - \psi (z)  \right) = y - \nabla \psi (z) =0$即得）。因此<br>$$<br>\psi^{</em>}(y) + \psi (y) = \left&lt; x, y \right&gt; \quad \Leftrightarrow \quad y = \nabla \psi (x).<br>\tag{7}<br>$$<br>因为$\psi = \psi^{<strong>}$，因此$\psi^{*}(y) + \psi^{</strong>} (y) = \left&lt; x, y \right&gt;$，也就是说<br>$$<br>\psi^{<em>*}(y) = \sup_{z \in \Omega} { \left&lt; z, y \right&gt; - \psi^{</em>} (z) }<br>\tag{8}<br>$$<br>在$y$处取得最大值，即$x = \nabla^{<em>} \psi (y)$。因此$(\nabla \psi^{</em>}) (\nabla \psi (x) ) = \nabla \psi^{*} (y) = x$，证毕。</p>
<p>再证第二等式。根据式(6)有$\psi^{<em>}( \nabla \psi (y) ) = \sup_{z \in \Omega} { \left&lt; z, \nabla \psi (y) \right&gt; - \psi (z) }$，同样的，令$\frac{\partial}{\partial z} \left( \left&lt; z, \nabla \psi (y) \right&gt; - \psi (z) \right) =0$，得到$\psi^{</em>}( \nabla \psi (y) ) = \left&lt; y, \nabla \psi (y) \right&gt; - \psi (y)$。同理有$\psi^{<em>}( \nabla \psi (x) ) = \left&lt; x, \nabla \psi (x) \right&gt; - \psi (x)$。那么<br>$$<br>\begin{aligned}<br>    \text{Div}_{\psi^{</em>}} (\nabla \psi (y), \nabla \psi (x)) &amp;= \psi^{<em>}(\nabla \psi (y)) - \psi^{</em>}(\nabla \psi (x)) - \left&lt; \nabla \psi^{*} (\nabla \psi (x)), \nabla \psi (y) - \nabla \psi (x) \right&gt; \<br>    &amp; = \left&lt; y, \nabla \psi (y) \right&gt; - \psi (y) - \left&lt; x, \nabla \psi (x) \right&gt; + \psi (x) - \left&lt; x, \nabla \psi (y) - \nabla \psi (x) \right&gt; \<br>    &amp;= \left&lt; y, \nabla \psi (y) \right&gt; - \psi (y) - \left&lt; x, \nabla \psi (x) \right&gt; + \psi (x) - \left&lt; x, \nabla \psi (y) \right&gt;  + \left&lt; x, \nabla \psi (x) \right&gt; \<br>    &amp;= \psi (x) - \psi (y) - \left&lt; x - y, \nabla \psi (y) \right&gt;\<br>    &amp;= \text{Div}_{\psi} (x, y).<br>\end{aligned}<br>\tag{9}<br>$$</p>
<p><code>10.</code> 分布的平均值。假设$U$是开集$S$服从$\mu$分布的随机变量，那么<br>$$<br>\min_{x \in S} \mathbb{E}<em>{U \sim \mu} \left[ \text{Div}</em>{\psi} (U, x) \right].<br>\tag{10}<br>$$<br>在$\bar{\mu} := \mathbb{E}<em>{\mu} = \int</em>{u \in S} u \mu(u)$处取得最小值。</p>
<p><em>证明：</em> 对于任意的$x \in S$，都有：<br>$$<br>\begin{aligned}<br>    &amp; \mathbb{E}<em>{U \sim \mu} \left[ \text{Div}</em>{\psi} (U, x) \right] - \mathbb{E}<em>{U \sim \mu} \left[ \text{Div}</em>{\psi} (U, \bar{\mu}) \right] \<br>    =&amp; \mathbb{E}<em>{U \sim \mu} \left[ \psi(U) - \psi(x) - \left&lt; \nabla \psi (x), U - x \right&gt; - \psi(U) + \psi(\bar{\mu}) + \left&lt; \nabla \psi (\bar{\mu}), U - \bar{\mu} \right&gt; \right] \<br>    =&amp; \psi(\bar{\mu}) - \psi(x) + x^T \nabla \psi (x) - \bar{\mu}^T \nabla \psi (\bar{\mu}) + \mathbb{E}</em>{U \sim \mu} \left[ - U^T \nabla \psi (x) + U^T \nabla \psi (\bar{\mu}) \right] \<br>    =&amp; \psi(\bar{\mu}) - \psi(x) - (\bar{\mu} - x)^T \nabla \psi (x) + \bar{\mu}^T \nabla \psi (x)- \bar{\mu}^T \nabla \psi (\bar{\mu}) \<br>    &amp;- (\mathbb{E}<em>{U \sim \mu} \left[ U \right])^T \nabla \psi (x) + (\mathbb{E}</em>{U \sim \mu} \left[ U \right])^T \nabla \psi (\bar{\mu})\<br>    =&amp; \psi(\bar{\mu}) - \psi(x) - (\bar{\mu} - x)^T \nabla \psi (x) \<br>    =&amp; \text{Div}_{\psi} (\bar{\mu}, x).<br>\end{aligned}<br>\tag{11}<br>$$<br>上式是非负的，当且仅当$x = \bar{\mu}$时为0。</p>
<p><code>11.</code> 毕达哥拉斯定理（三点定理）。如果$x^{<em>}$是$x_0$在凸集$C \in \Omega$的投影：<br>$$<br>x^{</em>} = \argmin_{x \in C} \text{Div}<em>{\psi} (x, x_0).<br>\tag{12}<br>$$<br>那么<br>$$<br>\text{Div}</em>{\psi} (y, x_0) \geq \text{Div}<em>{\psi} (y, x^{*}) + \text{Div}</em>{\psi} (x^{<em>}, x_0).<br>\tag{13}<br>$$<br>如果是欧式的情况下，上式的意味着$\angle y x^{</em>} x_0$是钝角。更为一般的情况，为引理1。</p>
<p><strong>引理1：</strong> 假设$L$是一个合适的函数(proper convex function)，其定义域是一个包含$C$的开集。$L$不一定是可微的。令$x^{<em>}$为：<br>$$<br>x^{</em>} = \argmin_{x \in C} { L(x) + \text{Div}<em>{\psi} (x, x_0) }.<br>\tag{14}<br>$$<br>那么对于任意的$y \in C$,有<br>$$<br>L(y) + \text{Div}</em>{\psi} (y, x_0) \geq L(x^{<em>}) + \text{Div}_{\psi} (x^{</em>}, x_0) + \text{Div}_{\psi} (y, x^{*}).<br>\tag{15}<br>$$<br>式(12)是$L=0$的特殊情况。这个性质是分析Bregman散度相关优化算法的关键。</p>
<p><em>证明：</em> 记$J(x) = L(x) + \text{Div}_{\psi} (x, x_0)$，因为$x^{<em>}$是$J$在$C$上的极小值点，因此存在<code>次梯度(subgradient)</code>$d \in \partial J(x^{</em>})$（<em>这里使用次梯度是因为$L$不一定可微*），使得<br>$$<br>\left&lt; d, x - x^{</em>} \right&gt; \geq 0, \quad x \in C.<br>\tag{16}<br>$$</p>
<p>因为<br>$$<br>\begin{aligned}<br>    \partial J(x^{<em>}) &amp;= { g + \nabla_{x = x^{</em>}} \text{Div}_{\psi} (x, x_0) : g \in \partial L(x^{<em>})}  \<br>    &amp;= { g + \nabla \psi (x^{</em>}) - \nabla \psi (x_0) : g \in \partial L(x^{<em>})}.<br>\end{aligned}<br>\tag{17}<br>$$<br>因此，必然存在一个次梯度$g \in \partial L(x^{</em>})$使得：<br>$$<br>\left&lt; g + \nabla \psi (x^{<em>}) - \nabla \psi (x_0), x - x^{</em>} \right&gt; \geq 0, \quad x \in C.<br>\tag{18}<br>$$</p>
<p>对于任意的$y \in C$，根据次梯度的性质有：<br>$$<br>\begin{aligned}<br>    L(y) \overset{次梯度性质}{\geq}&amp; L(x^{<em>}) + \left&lt; g, y - x^{</em>} \right&gt; \<br>    \overset{式(18)}{\geq}&amp; L(x^{<em>}) + \left&lt; \nabla \psi (x^{</em>}) - \nabla \psi (x_0), y - x^{<em>} \right&gt; \<br>    \overset{分凑项}{=}&amp; L(x^{</em>}) - \left&lt; \nabla \psi (x_0), x^{<em>} - x_0 \right&gt; + \psi (x^{</em>}) - \psi (x_0) \<br>    &amp;+ \left&lt; \nabla \psi (x_0), y - x_0 \right&gt; - \psi (y) + \psi (x_0) \<br>    &amp;- \left&lt; \nabla \psi (x^{<em>}), y - x^{</em>} \right&gt; + \psi (y) - \psi (x^{<em>}) \<br>    =&amp; L(x^{</em>}) + \text{Div}<em>{\psi} (x^{*}, x_0) - \text{Div}</em>{\psi} (y, x_0) + \text{Div}<em>{\psi} (y, x^{*}).<br>\end{aligned}<br>\tag{19}<br>$$<br>将$\text{Div}</em>{\psi} (y, x_0)$移动到不等式左边，完成证明。</p>
<hr>
<h2 id="II-Mirror-Descent"><a href="#II-Mirror-Descent" class="headerlink" title="II. Mirror Descent"></a>II. Mirror Descent</h2><p>次梯度下降的收敛速度通常取决于问题的维数。假设求函数$f$在$C$上的最小值，那么<code>次梯度下降(subgradient descent)</code>为<br>$$<br>\begin{aligned}<br>    x_{k+\frac{1}{2}} &amp;= x_k - \alpha_k g_k, \quad g_k \in \partial f(x_k) \<br>    x_{k+1} &amp;= \argmin_{x \in C} \frac{1}{2} | x - x_{k+\frac{1}{2}} |^2 = \argmin_{x \in C} \frac{1}{2} | x - \left( x_k - \alpha_k g_k \right) |^2.<br>\end{aligned}<br>\tag{20}<br>$$<br>可以解释如下。用$f$在$x_k$附近的一阶Taylor展开式近似$f$：<br>$$<br>f(x) \approx f(x_k) + \left&lt; g_k, x - x_k \right&gt;.<br>\tag{21}<br>$$<br>然后用$\frac{1}{2 \alpha_k} | x - x_k |^2$余项作为惩罚项。因此，更新规则是找到下式的极小值：<br>$$<br>x_{k+1} = \argmin_{x \in C} \left{ f(x_k) + \left&lt; g_k, x - x_k \right&gt; + \frac{1}{2 \alpha_k} | x - x_k |^2 \right}.<br>\tag{22}<br>$$<br>上式(22)与式(20)等价。为了将方法推广到欧几里得距离以外，可以直接使用Bregman散度作为余项的量度：<br>$$<br>\begin{aligned}<br>    x_{k + 1} &amp;= \argmin_{x \in C} \left{ f(x_k) + \left&lt; g_k, x - x_k \right&gt; + \frac{1}{\alpha_k} \text{Div}<em>{\psi}(x, x_k) \right} \<br>    &amp;= \argmin</em>{x \in C} \left{ \alpha_k f(x_k) + \alpha_k \left&lt; g_k, x - x_k \right&gt; + \text{Div}<em>{\psi}(x, x_k)  \right} \<br>    &amp;= \argmin</em>{x \in C} \left{ \left&lt; \alpha_k g_k, x \right&gt; + \text{Div}_{\psi}(x, x_k)  \right}.<br>\end{aligned}<br>\tag{23}<br>$$</p>
<p><strong>镜像梯度的解释</strong> </p>
<p>假设约束集$C$是整个空间（即无约束）。那么我们可以用关于$x$的梯度，寻找最优条件：<br>$$<br>\begin{aligned}<br>    &amp;  \frac{\partial}{\partial x} \left( \left&lt; g_k, x \right&gt; + \frac{1}{\alpha_k} \text{Div}<em>{\psi}(x, x_k)  \right) |</em>{x = x_{k+1}} = g_k + \frac{1}{\alpha} \left( \nabla \psi(x_{k+1}) - \nabla \psi(x_{k}) \right) = 0 \<br>    \Leftrightarrow &amp; \nabla \psi(x_{k+1}) = \nabla \psi(x_{k}) - \left&lt; \alpha_k g_k, x \right&gt; \<br>    \Leftrightarrow &amp; x_{k+1} = \left( \nabla \psi \right)^{-1} \left( \nabla \psi(x_{k}) - \left&lt; \alpha_k g_k, x \right&gt; \right) = \left( \nabla \psi^{*} \right) \left( \nabla \psi(x_{k}) - \left&lt; \alpha_k g_k, x \right&gt; \right).<br>\end{aligned}<br>\tag{24}<br>$$<br>如果是KL散度，那么$\nabla_{x_k(i)} \psi(x_{k}) = \log x_k(i) + 1$，更新规则为：<br>$$<br>x_{k+1}(i) = x_{k}(i) \exp \left( - \alpha_k g_k(i) \right).<br>\tag{25}<br>$$</p>
<p><strong>收敛速度</strong> </p>
<p>回顾在无约束的子梯度下降中，4个步骤：</p>
<p><code>1.</code> 受单次更新的约束:<br>$$<br>\begin{aligned}<br>| x_{k+1} - x^{<em>} |<em>2^2 =&amp; | x</em>{k} - \alpha_k g_k - x^{</em>} |<em>2^2 \<br>=&amp; | x</em>{k} - x^{<em>} |_2^2 - 2 \alpha_k \left&lt; g_k, x_k - x^{</em>} \right&gt; + \alpha_k^2 | g_k |<em>2^2 \<br>\leq &amp; | x</em>{k} - x^{<em>} |_2^2 - 2 \alpha_k \left( f(x_k) - f(x^{</em>}) \right) + \alpha_k^2 | g_k |_2^2.<br>\end{aligned}<br>\tag{26}<br>$$<br>上式用到了$f(x^{<em>}) \geq f(x_k) + \left&lt; g_k, x^{</em>} - x_k \right&gt;$。</p>
<p><code>2.</code> 递推求和：<br>$$<br>| x_{T+1} - x^{<em>} |<em>2^2 \leq | x</em>{1} - x^{</em>} |<em>2^2 - 2 \sum</em>{k=1}^{T} \alpha_k \left( f(x_k) - f(x^{*}) \right) + \sum_{k=1}^{T} \alpha_k^2 | g_k |_2^2.<br>\tag{27}<br>$$</p>
<p><code>3.</code> 根据$| x_{1} - x^{<em>} |<em>2^2 \leq R^2$和$| g_k |_2^2 \leq G^2$：<br>$$<br>2 \sum</em>{k=1}^{T} \alpha_k \left( f(x_k) - f(x^{</em>}) \right) \leq R^2 + G^2 \sum_{k=1}^{T} \alpha_k^2.<br>\tag{28}<br>$$</p>
<p><code>4.</code> 记$\epsilon_k = f(x_k) - f(x^{*})$，那么：<br>$$<br>\min_{k \in { 1, \cdots, T }} \epsilon_k \leq \frac{R^2 + G^2 \sum_{k=1}^{T} \alpha_k^2}{2 \sum_{k=1}^{T} \alpha_k}.<br>\tag{29}<br>$$<br>上式右边：<br>$$<br>\frac{R^2 + G^2 \sum_{k=1}^{T} \alpha_k^2}{2 \sum_{k=1}^{T} \alpha_k}<br>\geq \frac{2 RG \sqrt{\sum_{k=1}^{T} \alpha_k^2}}{2 \sum_{k=1}^{T} \alpha_k}<br>\geq \frac{RG}{\sqrt{T}} \frac{\sqrt{T} \sqrt{\sum_{k=1}^{T} \alpha_k^2}}{\sum_{k=1}^{T} \alpha_k}.<br>\tag{30}<br>$$<br>通过选择合适的步长$\alpha_k$，满足$T \geq \sum_{k=1}^{T} \alpha_k^2 \left( \sum_{k=1}^{T} \alpha_k \right)^2$，有：<br>$$<br>\min_{k \in { 1, \cdots, T }} \epsilon_k \leq \frac{RG}{\sqrt{T}}.<br>\tag{31}<br>$$<br>假设$C$是simplex，那么$R \leq \sqrt{2}$。如果每个梯度$g_i$的每个坐标都是以$M$为上界，那么$G$最多可以是$M\sqrt{n}$，即取决于维度。</p>
<p>从步骤2到4，可以用$\text{Div}<em>{\psi}(x^{*},x</em>{k+1})$代替$| x_{k+1} - x^{*} |_2^2$。而步骤1，则需要用到引理1。</p>
<p>假设$\psi$是$\sigma$-严格凸函数，将式(23)中$\alpha_k f(x_k) + \alpha_k \left&lt; g_k, x - x_k \right&gt;$的视为引理1中的$L$，那么：<br>$$<br>\begin{aligned}<br>    &amp; \alpha_k f(x_k) + \alpha_k \left&lt; g_k, x^{<em>} - x_k \right&gt; + \text{Div}_{\psi}(x^{</em>}, x_k) \<br>    \geq &amp; \alpha_k f(x_k) + \alpha_k \left&lt; g_k, x_{k+1} - x_k \right&gt; + \text{Div}<em>{\psi}(x</em>{k+1}, x_{k}) + \text{Div}<em>{\psi}(x^{*}, x</em>{k+1}).<br>\end{aligned}<br>\tag{32}<br>$$<br>移项后得：<br>$$<br>\begin{aligned}<br>\text{Div}<em>{\psi}(x^{*}, x</em>{k+1}) \leq &amp; \text{Div}<em>{\psi}(x^{<em>}, x_k) + \alpha_k \left&lt; g_k, x^{</em>} - x</em>{k+1} \right&gt; - \text{Div}<em>{\psi}(x</em>{k+1}, x_{k}) \<br>\overset{式(4)}{=} &amp; \text{Div}<em>{\psi}(x^{<em>}, x_k) + \alpha_k \left&lt; g_k, x^{</em>} - x</em>{k} \right&gt; + \alpha_k \left&lt; g_k, x_{k} - x_{k+1} \right&gt; - \text{Div}<em>{\psi}(x</em>{k+1}, x_{k}) \<br>\leq &amp; \text{Div}<em>{\psi}(x^{*}, x_k) - \alpha_k \left( f(x</em>{k}) - f(x^{<em>})\right) + \alpha_k \left&lt; g_k, x_{k} - x_{k+1} \right&gt; - \frac{\sigma}{2} | x_{k} - x_{k+1} |^2 \<br>\leq &amp; \text{Div}_{\psi}(x^{</em>}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{<em>})\right) + \alpha_k | g_k |_{</em>} | x_{k} - x_{k+1} | - \frac{\sigma}{2} | x_{k} - x_{k+1} |^2 \<br>\leq &amp; \text{Div}<em>{\psi}(x^{*}, x_k) - \alpha_k \left( f(x</em>{k}) - f(x^{<em>}) \right) + \frac{ \alpha_k^2 }{2 \sigma} | g_{k} |^2.<br>\end{aligned}<br>\tag{33}<br>$$<br>与式(26)对比，可以将$| x_{k} - x^{</em>} |<em>2^2$替换为$\text{Div}</em>{\psi}(x^{<em>}, x_{k})$。同样，假设$\text{Div}_{\psi}(x^{</em>}, x_{1})$界为$R^2$，且$| g_k |<em>{*}$的界为$G$，其中$| \cdot |</em>{*}$是对偶范数。</p>
<p>为了显示mirror descent的优势，假设$C$是$n$维的simplex，使用kL散度，其中$\psi$是关于$l_1$范数的1-严格凸函数。那么$l_1$范数的对偶范数就是$l_{\infty}$范数。因此，可以用kL散度考虑$\text{Div}<em>{\psi}(x^{*}, x</em>{1})$的界为$\log n$，而$G$则上界为$M$。所以，对于$RG$的值，mirror descent比子梯度下降小一个数量级$O(\sqrt{\frac{n}{\log n}})$。</p>
<p><strong>加速1：$f$是强凸函数。</strong>  我们说关于另一个函数$\psi$和模数$\lambda$，$f$是<code>强凸函数</code>，那么满足：<br>$$<br>f(x) \geq f(y) + \left&lt; g, x - y \right&gt; + \lambda \text{Div}_{\psi}(x, y) \quad g \in \partial f(y).<br>\tag{34}<br>$$</p>
<p>注意，并不要求$f$是可微的。那么式(33)可以增加强凸的条件：<br>$$<br>\begin{aligned}<br>\text{Div}<em>{\psi}(x^{*}, x</em>{k+1}) \leq &amp; \text{Div}<em>{\psi}(x^{<em>}, x_k) + \alpha_k \left&lt; g_k, x^{</em>} - x</em>{k+1} \right&gt; - \text{Div}<em>{\psi}(x</em>{k+1}, x_{k}) \<br>\overset{式(4)}{=} &amp; \text{Div}<em>{\psi}(x^{<em>}, x_k) + \alpha_k \left&lt; g_k, x^{</em>} - x</em>{k} \right&gt; + \alpha_k \left&lt; g_k, x_{k} - x_{k+1} \right&gt; - \text{Div}<em>{\psi}(x</em>{k+1}, x_{k}) \<br>\leq &amp; \text{Div}<em>{\psi}(x^{*}, x_k) - \alpha_k \left( f(x</em>{k}) - f(x^{<em>}) + \lambda \text{Div}_{\psi}(x^{</em>}, x_{k}) \right) + \alpha_k \left&lt; g_k, x_{k} - x_{k+1} \right&gt; - \frac{\sigma}{2} | x_{k} - x_{k+1} |^2 \<br>\leq &amp; \text{Div}<em>{\psi}(x^{*}, x_k) - \alpha_k \left( f(x</em>{k}) - f(x^{<em>}) + \lambda \text{Div}_{\psi}(x^{</em>}, x_{k}) \right) + \alpha_k | g_k |<em>{*} | x</em>{k} - x_{k+1} | - \frac{\sigma}{2} | x_{k} - x_{k+1} |^2 \<br>\leq &amp; (1 - \lambda \alpha_k ) \text{Div}<em>{\psi}(x^{*}, x_k) - \alpha_k \left( f(x</em>{k}) - f(x^{<em>}) \right) + \frac{ \alpha_k^2 }{2 \sigma} | g_{k} |_{</em>}^2.<br>\end{aligned}<br>\tag{35}<br>$$</p>
<p>记$\delta_{k} = \text{Div}<em>{\psi}(x^{*}, x_k)$，令$\alpha_k = \frac{1}{\lambda k}$，那么上式有：<br>$$<br>\begin{aligned}<br>&amp; \delta</em>{k+1} \leq \frac{k-1}{k} \delta_{k} - \frac{1}{\lambda k} \epsilon_{k} + \frac{G^2}{2 \sigma \lambda^2 k^2} \<br>\Rightarrow &amp; k \delta_{k+1} \leq (k-1) \delta_{k} - \frac{1}{\lambda} \epsilon_{k} + \frac{G^2}{2 \sigma \lambda^2 k}.<br>\end{aligned}<br>\tag{36}<br>$$<br>递推求和有：<br>$$<br>\begin{aligned}<br>&amp; T \delta_{T+1} \leq - \frac{1}{\lambda} \sum_{k=1}^{T} \epsilon_{k} + \frac{G^2}{2 \sigma \lambda^2} \sum_{k=1}^{T} \frac{1}{k}. \<br>\Rightarrow &amp; \min_{i \in { 1,\cdot, T }} \epsilon_{k} \leq \frac{G^2}{2 \sigma \lambda } \frac{1}{T} \sum_{k=1}^{T} \frac{1}{k} \leq<br>\frac{G^2}{2 \sigma \lambda } \frac{O(\log T)}{T}.<br>\end{aligned}<br>\tag{37}<br>$$</p>
<p><strong>加速2：$f$的梯度Lipschitz连续。</strong> 如果函数$f$的梯度是<code>Lipschitz连续</code>，那么存在$L &gt; 0$使得：<br>$$<br>| \nabla f(x) - \nabla f(y) |<em>{<em>} \leq L | x - y |, \quad \forall x, y.<br>\tag{38}<br>$$<br>有时候我们直接说$f$是光滑的。上式等价于：<br>$$<br>f(x) \leq f(y) + \left&lt; \nabla f(y), x - y \right&gt; + \frac{L}{2} | x - y |^2.<br>\tag{39}<br>$$<br>现在考虑式(33)中的$\left&lt; g_k, x^{</em>} - x</em>{k+1} \right&gt;$的界：<br>$$<br>\begin{aligned}<br>\left&lt; g_k, x^{<em>} - x_{k+1} \right&gt; =&amp; \left&lt; g_k, x^{</em>} - x_{k} \right&gt; + \left&lt; g_k, x_{k} - x_{k+1} \right&gt; \<br>\leq &amp; f(x^{<em>}) - f(x_k) +  \frac{L}{2} | x^{</em>} - x_k |^2 + f(x_{k}) - f(x_{k+1}) + \frac{L}{2} | x_{k} - x_{k+1} |^2  \<br>= &amp; f(x^{<em>}) - f(x_{k+1}) + \frac{L}{2} | x_{k} - x_{k+1} |^2.<br>\end{aligned}<br>\tag{40}<br>$$<br>将其代入式(33)有：<br>$$<br>\begin{aligned}<br>\text{Div}_{\psi}(x^{</em>}, x_{k+1}) \leq &amp; \text{Div}<em>{\psi}(x^{<em>}, x_k) + \alpha_k \left( f(x^{</em>}) - f(x</em>{k+1}) + \frac{L}{2} | x_{k} - x_{k+1} |^2 \right) - \text{Div}<em>{\psi}(x</em>{k+1}, x_{k}) \<br>\overset{式(4)}{=}&amp; \text{Div}<em>{\psi}(x^{<em>}, x_k) + \alpha_k \left( f(x^{</em>}) - f(x</em>{k+1}) + \frac{L}{2} | x_{k} - x_{k+1} |^2 \right) - \frac{\sigma}{2} | x_{k} - x_{k+1} |^2.<br>\end{aligned}<br>\tag{41}<br>$$<br>令$\alpha_k = \frac{\sigma}{L}$，有：<br>$$<br>\text{Div}<em>{\psi}(x^{*}, x</em>{k+1}) \leq \text{Div}<em>{\psi}(x^{*}, x_k) - \frac{\sigma}{L} \left( f(x</em>{k+1}) - f(x^{<em>}) \right).<br>\tag{42}<br>$$<br>递推之：<br>$$<br>\min_{k \in {2,\cdots, T+1}} f(x_k) - f(x^{</em>}) \leq \frac{L \text{Div}_{\psi}(x^{*}, x_1)}{\sigma T} \leq \frac{L R^2}{\sigma T}.<br>\tag{43}<br>$$<br>这时候的收敛速度为$O(\frac{1}{T})$，如果使用像Nesterov的技术，可以达到$O(\frac{1}{T^2})$。可以使用引理1证明，我们称之为<code>加速近似梯度法(accelerated proximal gradient method, APGM)</code>。</p>
<hr>
<h3 id="2-1-组合目标函数"><a href="#2-1-组合目标函数" class="headerlink" title="2.1 组合目标函数"></a>2.1 组合目标函数</h3><p>假设目标函数为$h(x) = f(x) + r(x)$，其中$f$是光滑的，而$r(x)$是simple的，比如$| x |<em>1$。如果直接使用上面的优化方式，可以得到收敛速度为$O(\frac{1}{\sqrt{T}})$，因为$h$不是光滑的。我们希望能够获得光滑时的收敛速度$O(\frac{1}{T})$是可以办到的，因为$r(x)$是简单的函数，只需要扩展式(23)如下：<br>$$<br>\begin{aligned}<br>    x</em>{k + 1} &amp;= \argmin_{x \in C} \left{ f(x_k) + \left&lt; g_k, x - x_k \right&gt; + r(x) + \frac{1}{\alpha_k} \text{Div}<em>{\psi}(x, x_k) \right} \<br>    &amp;= \argmin</em>{x \in C} \left{ \alpha_k f(x_k) + \alpha_k \left&lt; g_k, x - x_k \right&gt; + r(x) + \text{Div}<em>{\psi}(x, x_k)  \right} \<br>    &amp;= \argmin</em>{x \in C} \left{ \left&lt; \alpha_k g_k, x \right&gt; + r(x) + \text{Div}_{\psi}(x, x_k)  \right}.<br>\end{aligned}<br>\tag{44}<br>$$<br>这里我们只采用了$f$在$x_k$附近的一阶近似，不考虑$r(x)$。假设这个近似操作可以有效地计算，那么我们就可以证明上述的速率都能延续。</p>
<p>这里考虑更为一般的情况，即$f$是非光滑或者强凸的，如果$f$的梯度是Lipschitz连续，依然能够获得$O(\frac{1}{T^2})$的收敛速度。</p>
<p>将$\alpha_k f(x_k) + \alpha_k \left&lt; g_k, x - x_k \right&gt; + r(x)$视作引理1中的$L$，那么有：<br>$$<br>\begin{aligned}<br>&amp; \alpha_k f(x_k) + \alpha_k \left&lt; g_k, x^{<em>} - x_k \right&gt; + r(x^{</em>}) + \text{Div}<em>{\psi}(x^{*}, x_k) \<br>\geq &amp; \alpha_k f(x_k) + \alpha_k \left&lt; g_k, x</em>{k+1} - x_k \right&gt; + r(x_{k+1}) + \text{Div}<em>{\psi}(x</em>{k+1}, x_k) + \text{Div}<em>{\psi}(x^{*}, x</em>{k+1}).<br>\end{aligned}<br>\tag{45}<br>$$<br>那么，同式(33),有：<br>$$<br>\begin{aligned}<br>\text{Div}<em>{\psi}(x^{*}, x</em>{k+1}) \leq &amp; \text{Div}<em>{\psi}(x^{<em>}, x_k) + \alpha_k \left&lt; g_k, x^{</em>} - x</em>{k+1} \right&gt; + \alpha_k \left( r(x^{<em>}) - r(x_{k+1}) \right) - \text{Div}<em>{\psi}(x</em>{k+1}, x_{k}) \<br>\leq &amp; \cdots \<br>\leq &amp; \text{Div}_{\psi}(x^{</em>}, x_k) - \alpha_k \left( f(x_{k}) + r(x_{k+1}) - f(x^{<em>}) - f(r(x^{</em>}) ) \right) + \frac{ \alpha_k^2 }{2 \sigma} | g_{k} |^2.<br>\end{aligned}<br>\tag{46}<br>$$<br>记$\delta_k = \text{Div}<em>{\psi}(x^{*}, x_k)$，那么：<br>$$<br>f(x</em>{k}) + r(x_{k+1}) - f(x^{<em>}) - f(r(x^{</em>}) \leq \frac{1}{\alpha_k} ( \delta_k - \delta_{k+1} ) + \frac{ \alpha_k }{2 \sigma} | g_{k} |<em>{*}^2.<br>\tag{47}<br>$$<br>累加求和得到：<br>$$<br>\begin{aligned}<br>&amp; r(x</em>{T+1}) - r(x_1) + \sum_{k=1}^{T} \left( h(x_k) - h(x^{<em>}) \right) \<br>\leq &amp; \frac{\delta_1}{\alpha_1} + \sum_{k=2}^{T} \delta_k \left( \frac{1}{\alpha_k} - \frac{1}{\alpha_{k-1}} \right) - \frac{\delta_{T+1}}{\alpha_T} + \frac{ G^2 }{2 \sigma} \sum_{k=1}^{T} \alpha_k \<br>\leq &amp; R^2 \left( \frac{1}{\alpha_1} + \sum_{k=2}^{T} \left( \frac{1}{\alpha_k} - \frac{1}{\alpha_{k-1}} \right) \right) + \frac{ G^2 }{2 \sigma} \sum_{k=1}^{T} \alpha_k \<br>= &amp; \frac{R^2}{\alpha_T} + \frac{ G^2 }{2 \sigma} \sum_{k=1}^{T} \alpha_k.<br>\end{aligned}<br>\tag{48}<br>$$<br>假设选择$x_1 = \argmin_{x} r(X)$，那么有$r(x_{T+1}) - r(x_1) \geq 0$。令$\alpha_k = \frac{R}{G} \sqrt{\frac{\sigma}{k}}$，则有：<br>$$<br>\sum_{k=1}^{T} \left( h(x_k) - h(x^{</em>}) \right) \leq \frac{RG}{\sqrt{\sigma}} \left( \sqrt(T) + \frac{1}{2} \sum_{k=1}^{T} \frac{1}{\sqrt{k}} \right) = \frac{RG}{\sqrt{\sigma}} O(\sqrt{T}).<br>\tag{49}<br>$$<br>因此$\min_{k=1,\cdots,T} { h(x_k) - h(x^{*}) }$的收敛速度为$O(\frac{RG}{\sqrt{\sigma}T})$。</p>
<hr>
<h3 id="2-2-在线学习"><a href="#2-2-在线学习" class="headerlink" title="2.2 在线学习"></a>2.2 在线学习</h3><p><img src="Protocol_of_online_learning_1.jpg" alt=""></p>
<p>上图是在线学习算法。玩家在线学习的目的就是尽量减少遗憾，使得可能的损失$\sum_{k}f_k (x)$在所有可能的$x$上最小：<br>$$<br>\text{Regret} = \sum_{k=1}^Tf_k (x_k) - \min_{x} \sum_{k=1}^{T} f_k (x).<br>\tag{50}<br>$$</p>
<p>需要注意的是，没有假设对手如何选$f_k$，可以对立。在第$k$次迭代获得$f_k$后，使用mirror descent对$f_k$进行更新获得$x_{k+1}$：<br>$$<br>x_{k+1} = \argmin_{x \in C} \left{ f_k(x_k) + \left&lt; g_k, x - x_k \right&gt; + \frac{1}{\alpha_k} \text{Div}(x, x_k) \right}, \quad g_k \in \partial f_x (x_k).<br>\tag{51}<br>$$<br>可以推得regret有界。由式(33)有：<br>$$<br>f_k(x_k) - f_k(x^{<em>}) \leq \frac{1}{\alpha_k} \left( \text{Div}(x^{</em>}, x_k) - \text{Div}(x^{<em>}, x_{k+1}) \right) + \frac{\alpha_k}{2 \sigma} | g_k |_{</em>}^{2}.<br>\tag{52}<br>$$<br>从$k=1$开始累加，同式(48)和式(49)：<br>$$<br>\sum_{k=1}^{T} \left( f_k(x_k) - f_k(x^{*}) \right)\leq \frac{RG}{\sqrt{\sigma}} O(\sqrt{T}).<br>\tag{53}<br>$$<br>所以，regret的是以$O(\sqrt{T})$的速度增长。</p>
<p><strong>$f$是强凸函数</strong> 将$f_k$代替式(35)中的$f$，得到regret的界为$O(\log T)$。</p>
<p><strong>$f$的梯度是Lipschitz连续</strong> 式(43)的结果无法推广到在线学习的情况，如果用$f_k$代替$f$，那么会得到等式右边有$f_k(x_{k+1}) - f_k (x^{*})$。递推求和无法得到regret的界。因此，梯度的Lipschitz连续无法保证regret的界为$O(\log T)$。</p>
<p><strong>组合目标函数</strong> 在在线设定中，玩家和对手都有$r(x)$，对手在每次迭代时改变$f_k(x)$。每次迭代的目标函数是$h_k(x_k) = f_k(x_k) + r(x_k)$。更新规则为：<br>$$<br>x_{k+1} = \argmin_{x \in C} \left{ f_k(x_k) + \left&lt; g_k, x - x_k \right&gt;  + r(x) + \frac{1}{\alpha_k} \text{Div}(x, x_k) \right}, \quad g_k \in \partial f_x (x_k).<br>\tag{54}<br>$$<br>那么式(47)变为：<br>$$<br>f_k(x_{k}) + r(x_{k+1}) - f_k(x^{<em>}) - f_k(r(x^{</em>}) \leq \frac{1}{\alpha_k} ( \delta_k - \delta_{k+1} ) + \frac{ \alpha_k }{2 \sigma} | g_{k} |<em>{*}^2.<br>\tag{55}<br>$$<br>虽然我们这里用的是$r(x</em>{k+1})$，而不是$r(x_k)$，但这是没有问题的，因为$r$不会通过迭代而改变。<br>选择$x_1 = \argmin_{x} r(x)$，同式(48,49)有：<br>$$<br>\sum_{k=1}^{T} \left( h_k(x_k) - h_k(x^{*}) \right) \leq \frac{G}{\sqrt{\sigma}} O(\sqrt{T}).<br>\tag{56}<br>$$<br>因此，有$O(\sqrt{T})$。</p>
<p>当$f_k$为强凸时，我们可以得到组合情况下的$O(\log T)$regret。但是，同上面一样，$f$梯度的Lipschitz连续无法保证regret的界为$O(\log T)$。</p>
<hr>
<h3 id="2-3-随机优化"><a href="#2-3-随机优化" class="headerlink" title="2.3 随机优化"></a>2.3 随机优化</h3><p>让我们考虑优化一个函数，它的期望值形式为<br>$$<br>\min_{x} F(x) := \mathbb{E}<em>{w \sim p} [ f(x; w) ],<br>\tag{57}<br>$$<br>其中$p$是$w$的分布。这其中包含了很多机器学习模型。比如SVM的目标函数为：<br>$$<br>F(x) = \frac{1}{m} \sum</em>{i=1}^{m} \max \left{ 0, 1 - c_i \left&lt; a_i, x \right&gt; + \frac{\lambda}{2} | x |^2 \right}.<br>\tag{58}<br>$$<br>它可以解释成式(57)，$w$是在${ 1, 2, \cdots, m }$上的均匀分布，例如$p(w=i) = \frac{1}{m}$：<br>$$<br>f(x; i) = \max \left{ 0, 1 - c_i \left&lt; a_i, x \right&gt; \right} + \frac{\lambda}{2} | x |^2.<br>\tag{59}<br>$$<br>当$m$较大时，计算$F$及其子梯度的成本会很高。所以，一个简单的想法是基于一个随机选择的数据点进行更新。它可以认为是算法1中在线学习的一种特殊情况，步骤4中的对手现在随机选取$f_k$为$f(x;w_k)$，$w_k$与$p$无关。理想情况下，我们希望通过使用mirror descent更新，$x_k$将逐渐接近$F(x)$的最小化值。直观上这是非常合理的，通过使用$f_k$，我们可以计算出$F(x_k)$的无偏估计值和$F(x_k)$的次梯度(因为$w_k$是从$p$中进行独立同分布采样得到的)。这是随机优化的一种特殊情况，我们在算法2中进行了总结。</p>
<p><img src="Protocol_of_online_learning_2.jpg" alt=""></p>
<p>事实上，该方法在更一般的环境下也是有效的。为了简单起见，我们只说对手在迭代$k$时有$w_k$。那么在线学习算法$\mathcal{A}$就是简单的从一个有序集${ w_1, \cdots, w_k }$到$x_{k+1}$的确定性映射。将初始模型$x_1$表示为$\mathcal{A}(w_0)$。那么下面的定理就是在线到批量转换的关键。</p>
<p><strong>定理1</strong> 假设在线学习算法$\mathcal{A}$在使用算法1迭代$k$次后regret有上界$R_k$。假设$w_1, \cdots, w_{T+1}$独立同分布采样自$p$。定义$\hat{x}=\mathcal{A}(w_{j+1}, \cdots, w_{T})$，其中$j$均匀随机采样自${ 0, \cdots, T }$。那么：<br>$$<br>\mathbb{E}[F(\hat{x})] - \min_{x} F(x) \leq \frac{R_{T+1}}{T + 1}.<br>\tag{60}<br>$$<br>其中期望值与$w_1, \cdots, w_{T}$和$j$的随机性有关。</p>
<p>相似地，可以用$1 - \sigma$高概率保持边界：<br>$$<br>F(\hat{x}) - \min_{x} F(x) \leq \frac{R_{T+1}}{T + 1} \log \frac{1}{\sigma}.<br>\tag{61}<br>$$<br>其中概率与$w_1, \cdots, w_{T}$和$j$的随机性有关。</p>
<p><em>证明</em><br>$$<br>\begin{aligned}<br>\mathbb{E} [F(\hat{x})] =&amp; \mathbb{E}<em>{j,w_1, \cdots, w</em>{T+1}} [f(\hat{x}; w_{T+1})] \<br>=&amp; \mathbb{E}<em>{j,w_1, \cdots, w</em>{T+1}} [f(\mathcal{A}<em>{w</em>{j+1}, \cdots, w_{T}}; w_{T+1})] \<br>=&amp; \mathbb{E}<em>{w_1, \cdots, w</em>{T+1}} \left[ \frac{1}{T + 1} \sum_{j=0}^{T} f(\mathcal{A}<em>{w</em>{j+1}, \cdots, w_{T}}; w_{T+1}) \right] \quad (j \text{是均匀随机抽取的}) \<br>=&amp; \frac{1}{T + 1} \mathbb{E}<em>{w_1, \cdots, w</em>{T+1}} \left[ \sum_{j=0}^{T} f(\mathcal{A}<em>{w</em>{1}, \cdots, w_{T-j}}; w_{T+1-j}) \right] \quad (\text{平移指标}) \<br>=&amp; \frac{1}{T + 1} \mathbb{E}<em>{w_1, \cdots, w</em>{T+1}} \left[ \sum_{s=1}^{T+1} f(\mathcal{A}<em>{w</em>{1}, \cdots, w_{s-1}}; w_{s}) \right] \quad (\text{替换变量} s = T+1-j ) \<br>\leq &amp; \frac{1}{T + 1} \mathbb{E}<em>{w_1, \cdots, w</em>{T+1}} \left[ \min_{x} \sum_{s=1}^{T+1} f(x; w_{s}) + R_{T+1} \right] \quad (\text{利用regret的界}) \<br>\leq &amp; \min_{x} \mathbb{E}<em>{w} [f(x;w)] + \frac{R</em>{T+1}}{T+1} \quad (\text{最小值的期望值小于期望值的最小值}) \<br>= &amp; \min_{x} F(x) + \frac{R_{T+1}}{T + 1}.<br>\end{aligned}<br>\tag{62}<br>$$</p>
]]></content>
  </entry>
</search>
