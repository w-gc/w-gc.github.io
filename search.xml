<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Sobolev 梯度</title>
    <url>/2020/07/11/SOBOLEV%20GRADIENT/</url>
    <content><![CDATA[<h1 id="Sobolev-GRADIENT"><a href="#Sobolev-GRADIENT" class="headerlink" title="Sobolev GRADIENT"></a>Sobolev GRADIENT</h1><blockquote>
<p>翻译自 <a href="pu.edu.pk/images/journal/maths/PDF/Paper-11-48-2-16.pdf">Application of Sobolev Gradient Method to Solve Klein Gordon Equation</a></p>
</blockquote>
<p>实值$C^1$函数$F$在$\mathbb{R}^n$上的梯度$\nabla F$，其中$n$为正整数，由下式给出：</p>
<script type="math/tex; mode=display">
\lim_{t \rightarrow 0} \frac{1}{t} \left( F \left( x + t h \right) - F \left( x \right) \right) = F'(x) h = \left< h, \nabla F(x) \right>_{\mathbb{R}^n} \quad x, h \in \mathbb{R}^n.
\tag{1}</script><p>记$\left&lt; \cdot, \cdot \right&gt;_{S}$是$\mathbb{R}^n$上一个不同于标准内积$\left&lt; \cdot, \cdot \right&gt;_{\mathbb{R}^n}$的内积。那么，存在一个函数$\nabla_S F: \mathbb{R}^n \rightarrow \mathbb{R}^n$，使得</p>
<script type="math/tex; mode=display">
F'(x)h = \left< h, \nabla_S F(x) \right>_S \quad x, h \in \mathbb{R}^n.
\tag{2}</script><p>线性函数$F’(x)$可以用$\mathbb{R}^n$上的任意内积表示。那么$\nabla_S F(x)$就是函数$F$在内积为$\left&lt; \cdot, \cdot \right&gt;_{S}$下的梯度。考虑线性变换$A : \mathbb{R}^n \rightarrow \mathbb{R}^n$，那么两个内积的关系为：</p>
<script type="math/tex; mode=display">
\left< x, y \right>_{S} = \left< x, Ay \right>_{\mathbb{R}^n} \quad x, y \in \mathbb{R}^n.
\tag{3}</script><p>则两个梯度的关系为：</p>
<script type="math/tex; mode=display">
\left( \nabla_S F \right) (x) = A^{-1} \nabla F (x) , x \in \mathbb{R}^n \quad x \in \mathbb{R}^n.
\tag{4}</script><p>对于每个$x \in \mathbb{R}^n$在$\mathbb{R}^n$中都存在内积$\left&lt; \cdot, \cdot \right&gt;_{x}$。对于每个$x \in \mathbb{R}^n$，定义$\nabla_x F : \mathbb{R}^n \rightarrow \mathbb{R}^n$为：</p>
<script type="math/tex; mode=display">
F' (x) h = \left< h, \nabla_x F(x) \right>_x \quad \text{for } x, h \in \mathbb{R}^n.
\tag{5}</script><p>$F$的梯度有很多种，这取决于度量的选择，而这些梯度具有非常不同的数值特性。这种定义在有限或无限维Sobolev空间中的函数梯度，称为<code>Sobolev梯度</code>。最速下降可以分为两种类型：离散最速下降和连续最速下降。</p>
<p>设$\nabla_S F$为实值$C^1$函数$F$在Hilbert空间$H$上对为内积$\left&lt; \cdot, \cdot \right&gt;_S$的梯度。离散最速下降可以看作是，给定初始点$x_0$，构造一个序列$\{ x_k \}$：</p>
<script type="math/tex; mode=display">
x_k = x_{k-1} - \delta_k \left( \nabla F \right) ( x_{k-1} ), k = 1, 2, \cdots.
\tag{6}</script><p>其中：</p>
<script type="math/tex; mode=display">
\delta_k = \min_{\delta} F \left( x_{k-1} - \delta \left( \nabla F \right) ( x_{k-1} ) \right).
\tag{7}</script><p>连续最速下降是一个构造函数$z : [ 0, \infty ) \rightarrow H$：</p>
<script type="math/tex; mode=display">
\frac{\mathrm{d} z}{\mathrm{d} t} = - \nabla F \left( z(t) \right), z(0) = z_{\text{initial}}.
\tag{8}</script><p>$z(t) \rightarrow z_{\infty}$在$F$上的合适条件下，其中$F(z_{\infty})$是$F$的最小值。</p>
<p>离散最速下降的极限情况可以看作是连续最速下降，因此，我们可以把(6)看作是近似求解(8)的数值方案。 连续最速下降给出了一个理论起点，证明离散最速下降的收敛性。</p>
<p>考虑式(6)，当$u = \lim_{k \rightarrow \infty} x_k$，满足</p>
<script type="math/tex; mode=display">
F(u) = 0 \text{ or } (\nabla_S F) (u) = 0.
\tag{9}</script><p>同样，对于(8)，$u = \lim_{t \rightarrow \infty} x_t$亦满足(9)。</p>
]]></content>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Bregman Divergence and Mirror Descent</title>
    <url>/2020/07/11/Bregman%20Divergence%20and%20Mirror%20Descent/</url>
    <content><![CDATA[<h1 id="Bregman-Divergence-and-Mirror-Descent"><a href="#Bregman-Divergence-and-Mirror-Descent" class="headerlink" title="Bregman Divergence and Mirror Descent"></a>Bregman Divergence and Mirror Descent</h1><blockquote>
<p>[1] <a href="http://users.cecs.anu.edu.au/~xzhang/teaching/bregman.pdf" target="_blank" rel="noopener">Bregman Divergence and Mirror Descent</a></p>
</blockquote>
<hr>
<h2 id="I-Bregman-Divergence"><a href="#I-Bregman-Divergence" class="headerlink" title="I. Bregman Divergence"></a>I. Bregman Divergence</h2><p><strong>动机</strong></p>
<ul>
<li>将欧几里得距离的平方概括为一类距离，这些距离都具有相似的性质。</li>
<li>在机器学习、聚类、指数族等方面有很多应用。</li>
</ul>
<p><strong>定义1（Bregman divergence）</strong> 函数$\psi : \Omega \rightarrow \mathbb{R}$满足：<br>a). 严凸<br>b). 连续可微<br>c). 定义在一个封闭的凸集$\Omega$上。<br>那么<code>Bregman散度</code>可以定义为：</p>
<script type="math/tex; mode=display">
\text{Div}_{\psi} (x, y) = \psi(x) - \psi(y) - \left< \nabla \psi (y), x - y \right>.
\tag{1}</script><p>即为$\psi$在$x$处的值与$\psi$在$y$周围的一阶泰勒展开式取$x$的值之间的差。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>函数名</th>
<th>$\psi (x)$</th>
<th>函数定义域</th>
<th>$\text{Div}_{\psi} (x, y)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Squared norm</td>
<td>$\frac{1}{2}x^2$</td>
<td>$(-\infty, +\infty)$</td>
<td>$\frac{1}{2}(x-y)^2$</td>
</tr>
<tr>
<td>Shannon entropy</td>
<td>$x \log x - x$</td>
<td>$[0, +\infty)$</td>
<td>$x \log \frac{x}{y} - x +y$</td>
</tr>
<tr>
<td>Bit entropy</td>
<td>$x \log x  + (1 - x) \log (1 - x)$</td>
<td>$[0, 1]$</td>
<td>$x \log \frac{x}{y} + (1 - x) \log \frac{(1 - x)}{(1 - y)}$</td>
</tr>
<tr>
<td>Burg entropy</td>
<td>$- \log x$</td>
<td>$(0, +\infty)$</td>
<td>$\frac{x}{y} - \log \frac{x}{y} - 1$</td>
</tr>
<tr>
<td>Hellinger</td>
<td>$- \sqrt{1 - x^2}$</td>
<td>$[-1, 1]$</td>
<td>$(1 - xy)(1 - y^2)^{-\frac{1}{2}} - (1 - x^2 )^{\frac{1}{2}}$</td>
</tr>
<tr>
<td>$l_p$ quasi-norm</td>
<td>$- x^p \quad (0&lt;p&lt;1)$</td>
<td>$[0, +\infty)$</td>
<td>$-x^p+pxy^{p-1}-(p-1)y^p$</td>
</tr>
<tr>
<td>$l_p$ norm</td>
<td>$- \vert x \vert^p \quad (1&lt;p&lt;\infty)$</td>
<td>$(-\infty, +\infty)$</td>
<td>$\vert x \vert^p - p x y^{p-1} \text{sgn}(y) + (p-1) \vert y \vert^p$</td>
</tr>
<tr>
<td>Exponential</td>
<td>$\exp(x)$</td>
<td>$(-\infty, +\infty)$</td>
<td>$\exp(x) - \left(x - y + 1 \right) \exp(y)$</td>
</tr>
<tr>
<td>Hellinger</td>
<td>$\frac{1}{x}$</td>
<td>$(0, +\infty)$</td>
<td>$\frac{1}{x} + \frac{x}{y^2} - \frac{2}{y}$</td>
</tr>
</tbody>
</table>
</div>
<p><strong>例子</strong></p>
<ul>
<li>欧式距离。令$\psi(x)=\frac{1}{2}|x|^2$，则$\text{Div}_{\psi}(x,y) = \frac{1}{2} | x - y |^2$。</li>
<li>$\Omega = \{ x \in \mathbb{R}_{+}^{n} : \sum_{i} x_i = 1 \}$，且$\psi(x) = \sum_{i} x_i \log x_i$。那么$\text{Div}_{\psi}(x,y) = \sum_{i} x_i \log \frac{x_i}{y_i}, x, y \in \Omega$。这就是所谓的相对熵，或概率分布$x$和$y$之间的Kullback-Leibler散度。</li>
<li>$l_p$范式。令$p \geq 1$且$\frac{1}{p} + \frac{1}{q} = 1$，$\psi(x)=\frac{1}{2}|x|_q^2$。那么$\text{Div}_{\psi}(x,y) = \frac{1}{2}|x|_q^2 + \frac{1}{2}|y|_q^2 - \left&lt; x , \nabla \frac{1}{2}|y|_q^2 \right&gt;$。注意$\frac{1}{2}|y|_q^2$不一定是可连续微分的，这就使得这种情况与我们的定义不完全一致。</li>
</ul>
<p><strong>Properties of Bregman divergence</strong></p>
<p><code>1.</code> Bregman散度$\text{Div}_{\psi}(x,y)$关于第一个变量$x$是严格凸函数，那么函数$\psi$的严格凸性并不重要。</p>
<blockquote>
<p><em>原文：</em> Strict convexity in the first argument $x$. Trivial by the strict convexity of $\psi$.</p>
</blockquote>
<p><code>2.</code> 非负性。$\forall x, y$，有$\text{Div}_{\psi}(x,y) \geq 0$，等号当且仅当$x=y$时取得。不要求严格凸亦成立（Trivial by strict convexity.）。</p>
<p><code>3.</code> 非对称性。一般情况下$\text{Div}_{\psi}(x,y) \neq \text{Div}_{\psi}(y,x)$，例如KL散度。对称性并不是一直有用。</p>
<p><code>4.</code> 关于第二变量$y$非凸。令$\Omega = [1, \infty), \psi(x) = - \log x$。那么$\text{Div}_{\psi}(x,y) = - \log x + \log y + \frac{x-y}{y}$。计算其关于$y$的二阶偏导为$\frac{1}{y^2}(\frac{2x}{y}-1)$，当$2x &lt; y$时为负。</p>
<p><code>5.</code> 关于$\psi$的线性。对于任意$a &gt; 0$，有$\text{Div}_{\psi + a \phi}(x,y)= \text{Div}_{\psi}(x,y) + a \text{Div}_{\phi}(x,y)$。</p>
<p><code>6.</code> 关于$x$的梯度。$\frac{\partial }{ \partial x} \text{Div}_{\psi}(x,y) = \nabla \psi (x) - \nabla \psi (y)$。关于$y$的梯度比较棘手，不常用。</p>
<p><code>7.</code> 广义三角形不等式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \text{Div}_{\psi}(x,y) + \text{Div}_{\psi}(y,z) &= \psi(x) - \psi(y) - \left< \nabla \psi (y), x - y \right> + \psi(y) - \psi(z) - \left< \nabla \psi (z), y - z \right> \\
    &= \text{Div}_{\psi}(x,z) + \left< x - y, \nabla \psi (z) - \nabla \psi (y) \right>.
\end{aligned}
\tag{2}</script><p><code>8.</code> 特例，如果$\psi$对于某些范式和模数$\sigma$满足：</p>
<script type="math/tex; mode=display">
\psi (x) \geq \psi(y) + \left< \nabla \psi(y), x - y\right> + \frac{\sigma}{2} \| x - y \|^2.
\tag{3}</script><p>则称$\psi$是$\sigma$-<code>强凸函数</code>。需要注意的是，并不需要是欧几里得范式。如果是欧式范式，那么上式条件等价于$\psi(x) - \frac{\sigma}{2} | x |^2$是凸函数。比如，KL散度中的$\psi (x) = \sum_{i} x_i$是在$\Omega = \{ x \in \mathbb{R}_{+}^{n} : \sum_{i} x_i = 1 \}$上$l_1$范式下的1-强凸函数。当$\psi$是$\sigma$-强凸函数，有：</p>
<script type="math/tex; mode=display">
\text{Div}_{\psi}(x,y) \geq \frac{\sigma}{2} \| x - y \|^2.
\tag{4}</script><p><em>证明：</em> 根据定义有$\text{Div}_{\psi} (x, y) = \psi(x) - \psi(y) - \left&lt; \nabla \psi (y), x - y \right&gt; \geq \frac{\sigma}{2} | x - y |^2$。</p>
<p><code>9.</code> 对偶性。假设$\psi$是强凸函数，那么</p>
<script type="math/tex; mode=display">
(\nabla \psi^{*}) \nabla \psi (x) = x, \qquad \text{Div}_{\psi} (x, y) = \text{Div}_{\psi^{*}} (\nabla \psi (y), \nabla \psi (x)).
\tag{5}</script><p><em>证明：</em> 先证第一个等式。根据：</p>
<script type="math/tex; mode=display">
\psi^{*}(y) = \sup_{z \in \Omega} \{ \left< z, y \right> - \psi (z) \}.
\tag{6}</script><p>$\sup$是可以实现的，因为$\psi$是强凸的，且定义域$\Omega$是闭集。如果在$x$处取得最大值，当且仅当$y = \nabla \psi (x)$（令$\frac{\partial}{\partial z} \left( \left&lt; z, y \right&gt; - \psi (z)  \right) = y - \nabla \psi (z) =0$即得）。因此</p>
<script type="math/tex; mode=display">
\psi^{*}(y) + \psi (y) = \left< x, y \right> \quad \Leftrightarrow \quad y = \nabla \psi (x).
\tag{7}</script><p>因为$\psi = \psi^{<strong>}$，因此$\psi^{*}(y) + \psi^{</strong>} (y) = \left&lt; x, y \right&gt;$，也就是说</p>
<script type="math/tex; mode=display">
\psi^{**}(y) = \sup_{z \in \Omega} \{ \left< z, y \right> - \psi^{*} (z) \}
\tag{8}</script><p>在$y$处取得最大值，即$x = \nabla^{<em>} \psi (y)$。因此$(\nabla \psi^{</em>}) (\nabla \psi (x) ) = \nabla \psi^{*} (y) = x$，证毕。</p>
<p>再证第二等式。根据式(6)有$\psi^{<em>}( \nabla \psi (y) ) = \sup_{z \in \Omega} \{ \left&lt; z, \nabla \psi (y) \right&gt; - \psi (z) \}$，同样的，令$\frac{\partial}{\partial z} \left( \left&lt; z, \nabla \psi (y) \right&gt; - \psi (z) \right) =0$，便得到$\psi^{</em>}( \nabla \psi (y) ) = \left&lt; y, \nabla \psi (y) \right&gt; - \psi (y)$。</p>
<p>同理有$\psi^{*}( \nabla \psi (x) ) = \left&lt; x, \nabla \psi (x) \right&gt; - \psi (x)$。那么</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \text{Div}_{\psi^{*}} (\nabla \psi (y), \nabla \psi (x)) &= \psi^{*}(\nabla \psi (y)) - \psi^{*}(\nabla \psi (x)) - \left< \nabla \psi^{*} (\nabla \psi (x)), \nabla \psi (y) - \nabla \psi (x) \right> \\
    & = \left< y, \nabla \psi (y) \right> - \psi (y) - \left< x, \nabla \psi (x) \right> + \psi (x) - \left< x, \nabla \psi (y) - \nabla \psi (x) \right> \\
    &= \left< y, \nabla \psi (y) \right> - \psi (y) - \left< x, \nabla \psi (x) \right> + \psi (x) - \left< x, \nabla \psi (y) \right>  + \left< x, \nabla \psi (x) \right> \\
    &= \psi (x) - \psi (y) - \left< x - y, \nabla \psi (y) \right>\\
    &= \text{Div}_{\psi} (x, y).
\end{aligned}
\tag{9}</script><p><code>10.</code> 分布的平均值。假设$U$是开集$S$服从$\mu$分布的随机变量，那么</p>
<script type="math/tex; mode=display">
\min_{x \in S} \mathbb{E}_{U \sim \mu} \left[ \text{Div}_{\psi} (U, x) \right].
\tag{10}</script><p>在$\bar{\mu} := \mathbb{E}_{\mu} = \int_{u \in S} u \mu(u)$处取得最小值。</p>
<p><em>证明：</em> 对于任意的$x \in S$，都有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    & \mathbb{E}_{U \sim \mu} \left[ \text{Div}_{\psi} (U, x) \right] - \mathbb{E}_{U \sim \mu} \left[ \text{Div}_{\psi} (U, \bar{\mu}) \right] \\
    =& \mathbb{E}_{U \sim \mu} \left[ \psi(U) - \psi(x) - \left< \nabla \psi (x), U - x \right> - \psi(U) + \psi(\bar{\mu}) + \left< \nabla \psi (\bar{\mu}), U - \bar{\mu} \right> \right] \\
    =& \psi(\bar{\mu}) - \psi(x) + x^T \nabla \psi (x) - \bar{\mu}^T \nabla \psi (\bar{\mu}) + \mathbb{E}_{U \sim \mu} \left[ - U^T \nabla \psi (x) + U^T \nabla \psi (\bar{\mu}) \right] \\
    =& \psi(\bar{\mu}) - \psi(x) - (\bar{\mu} - x)^T \nabla \psi (x) + \bar{\mu}^T \nabla \psi (x)- \bar{\mu}^T \nabla \psi (\bar{\mu}) \\
    &- (\mathbb{E}_{U \sim \mu} \left[ U \right])^T \nabla \psi (x) + (\mathbb{E}_{U \sim \mu} \left[ U \right])^T \nabla \psi (\bar{\mu})\\
    =& \psi(\bar{\mu}) - \psi(x) - (\bar{\mu} - x)^T \nabla \psi (x) \\
    =& \text{Div}_{\psi} (\bar{\mu}, x).
\end{aligned}
\tag{11}</script><p>上式是非负的，当且仅当$x = \bar{\mu}$时为0。</p>
<p><code>11.</code> 毕达哥拉斯定理（三点定理）。如果$x^{*}$是$x_0$在凸集$C \in \Omega$的投影：</p>
<script type="math/tex; mode=display">
x^{*} = argmin_{x \in C} \text{Div}_{\psi} (x, x_0).
\tag{12}</script><p>那么</p>
<script type="math/tex; mode=display">
\text{Div}_{\psi} (y, x_0) \geq \text{Div}_{\psi} (y, x^{*}) + \text{Div}_{\psi} (x^{*}, x_0).
\tag{13}</script><p>如果是欧式的情况下，上式的意味着$\angle y x^{*} x_0$是钝角。更为一般的情况，为引理1。</p>
<p><strong>引理1：</strong> 假设$L$是一个合适的函数(proper convex function)，其定义域是一个包含$C$的开集。$L$不一定是可微的。令$x^{*}$为：</p>
<script type="math/tex; mode=display">
x^{*} = argmin_{x \in C} \{ L(x) + \text{Div}_{\psi} (x, x_0) \}.
\tag{14}</script><p>那么对于任意的$y \in C$,有</p>
<script type="math/tex; mode=display">
L(y) + \text{Div}_{\psi} (y, x_0) \geq L(x^{*}) + \text{Div}_{\psi} (x^{*}, x_0) + \text{Div}_{\psi} (y, x^{*}).
\tag{15}</script><p>式(12)是$L=0$的特殊情况。这个性质是分析Bregman散度相关优化算法的关键。</p>
<p><em>证明：</em> 记$J(x) = L(x) + \text{Div}_{\psi} (x, x_0)$，因为$x^{<em>}$是$J$在$C$上的极小值点，因此存在<code>次梯度(subgradient)</code>$d \in \partial J(x^{</em>})$（<em>这里使用次梯度是因为$L$不一定可微</em>），使得</p>
<script type="math/tex; mode=display">
\left< d, x - x^{*} \right> \geq 0, \quad x \in C.
\tag{16}</script><p>因为</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \partial J(x^{*}) &= \{ g + \nabla_{x = x^{*}} \text{Div}_{\psi} (x, x_0) : g \in \partial L(x^{*})\}  \\
    &= \{ g + \nabla \psi (x^{*}) - \nabla \psi (x_0) : g \in \partial L(x^{*})\}.
\end{aligned}
\tag{17}</script><p>因此，必然存在一个次梯度$g \in \partial L(x^{*})$使得：</p>
<script type="math/tex; mode=display">
\left< g + \nabla \psi (x^{*}) - \nabla \psi (x_0), x - x^{*} \right> \geq 0, \quad x \in C.
\tag{18}</script><p>对于任意的$y \in C$，根据次梯度的性质有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    L(y) \overset{次梯度性质}{\geq}& L(x^{*}) + \left< g, y - x^{*} \right> \\
    \overset{式(18)}{\geq}& L(x^{*}) + \left< \nabla \psi (x^{*}) - \nabla \psi (x_0), y - x^{*} \right> \\
    \overset{分凑项}{=}& L(x^{*}) - \left< \nabla \psi (x_0), x^{*} - x_0 \right> + \psi (x^{*}) - \psi (x_0) \\
    &+ \left< \nabla \psi (x_0), y - x_0 \right> - \psi (y) + \psi (x_0) \\
    &- \left< \nabla \psi (x^{*}), y - x^{*} \right> + \psi (y) - \psi (x^{*}) \\
    =& L(x^{*}) + \text{Div}_{\psi} (x^{*}, x_0) - \text{Div}_{\psi} (y, x_0) + \text{Div}_{\psi} (y, x^{*}).
\end{aligned}
\tag{19}</script><p>将$\text{Div}_{\psi} (y, x_0)$移动到不等式左边，完成证明。</p>
<hr>
<h2 id="II-Mirror-Descent"><a href="#II-Mirror-Descent" class="headerlink" title="II. Mirror Descent"></a>II. Mirror Descent</h2><p>次梯度下降的收敛速度通常取决于问题的维数。假设求函数$f$在$C$上的最小值，那么<code>次梯度下降(subgradient descent)</code>为</p>
<script type="math/tex; mode=display">
\begin{aligned}
    x_{k+\frac{1}{2}} &= x_k - \alpha_k g_k, \quad g_k \in \partial f(x_k) \\
    x_{k+1} &= argmin_{x \in C} \frac{1}{2} \| x - x_{k+\frac{1}{2}} \|^2 = argmin_{x \in C} \frac{1}{2} \| x - \left( x_k - \alpha_k g_k \right) \|^2.
\end{aligned}
\tag{20}</script><p>可以解释如下。用$f$在$x_k$附近的一阶Taylor展开式近似$f$：</p>
<script type="math/tex; mode=display">
f(x) \approx f(x_k) + \left< g_k, x - x_k \right>. 
\tag{21}</script><p>然后用$\frac{1}{2 \alpha_k} | x - x_k |^2$余项作为惩罚项。因此，更新规则是找到下式的极小值：</p>
<script type="math/tex; mode=display">
x_{k+1} = argmin_{x \in C} \left\{ f(x_k) + \left< g_k, x - x_k \right> + \frac{1}{2 \alpha_k} \| x - x_k \|^2 \right\}.
\tag{22}</script><p>上式(22)与式(20)等价。为了将方法推广到欧几里得距离以外，可以直接使用Bregman散度作为余项的量度：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    x_{k + 1} &= argmin_{x \in C} \left\{ f(x_k) + \left< g_k, x - x_k \right> + \frac{1}{\alpha_k} \text{Div}_{\psi}(x, x_k) \right\} \\
    &= argmin_{x \in C} \left\{ \alpha_k f(x_k) + \alpha_k \left< g_k, x - x_k \right> + \text{Div}_{\psi}(x, x_k)  \right\} \\
    &= argmin_{x \in C} \left\{ \left< \alpha_k g_k, x \right> + \text{Div}_{\psi}(x, x_k)  \right\}.
\end{aligned}
\tag{23}</script><p><strong>镜像梯度的解释</strong> </p>
<p>假设约束集$C$是整个空间（即无约束）。那么我们可以用关于$x$的梯度，寻找最优条件：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    &  \frac{\partial}{\partial x} \left( \left< g_k, x \right> + \frac{1}{\alpha_k} \text{Div}_{\psi}(x, x_k)  \right) |_{x = x_{k+1}} = g_k + \frac{1}{\alpha} \left( \nabla \psi(x_{k+1}) - \nabla \psi(x_{k}) \right) = 0 \\
    \Leftrightarrow & \nabla \psi(x_{k+1}) = \nabla \psi(x_{k}) - \left< \alpha_k g_k, x \right> \\
    \Leftrightarrow & x_{k+1} = \left( \nabla \psi \right)^{-1} \left( \nabla \psi(x_{k}) - \left< \alpha_k g_k, x \right> \right) = \left( \nabla \psi^{*} \right) \left( \nabla \psi(x_{k}) - \left< \alpha_k g_k, x \right> \right).
\end{aligned}
\tag{24}</script><p>如果是KL散度，那么$\nabla_{x_k(i)} \psi(x_{k}) = \log x_k(i) + 1$，更新规则为：</p>
<script type="math/tex; mode=display">
x_{k+1}(i) = x_{k}(i) \exp \left( - \alpha_k g_k(i) \right).
\tag{25}</script><p><strong>收敛速度</strong> </p>
<p>回顾在无约束的子梯度下降中，4个步骤：</p>
<p><code>1.</code> 受单次更新的约束:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\| x_{k+1} - x^{*} \|_2^2 =& \| x_{k} - \alpha_k g_k - x^{*} \|_2^2 \\
=& \| x_{k} - x^{*} \|_2^2 - 2 \alpha_k \left< g_k, x_k - x^{*} \right> + \alpha_k^2 \| g_k \|_2^2 \\
\leq & \| x_{k} - x^{*} \|_2^2 - 2 \alpha_k \left( f(x_k) - f(x^{*}) \right) + \alpha_k^2 \| g_k \|_2^2.
\end{aligned}
\tag{26}</script><p>上式用到了$f(x^{<em>}) \geq f(x_k) + \left&lt; g_k, x^{</em>} - x_k \right&gt;$。</p>
<p><code>2.</code> 递推求和：</p>
<script type="math/tex; mode=display">
\| x_{T+1} - x^{*} \|_2^2 \leq \| x_{1} - x^{*} \|_2^2 - 2 \sum_{k=1}^{T} \alpha_k \left( f(x_k) - f(x^{*}) \right) + \sum_{k=1}^{T} \alpha_k^2 \| g_k \|_2^2.
\tag{27}</script><p><code>3.</code> 根据$| x_{1} - x^{*} |_2^2 \leq R^2$和$| g_k |_2^2 \leq G^2$：</p>
<script type="math/tex; mode=display">
2 \sum_{k=1}^{T} \alpha_k \left( f(x_k) - f(x^{*}) \right) \leq R^2 + G^2 \sum_{k=1}^{T} \alpha_k^2.
\tag{28}</script><p><code>4.</code> 记$\epsilon_k = f(x_k) - f(x^{*})$，那么：</p>
<script type="math/tex; mode=display">
\min_{k \in \{ 1, \cdots, T \}} \epsilon_k \leq \frac{R^2 + G^2 \sum_{k=1}^{T} \alpha_k^2}{2 \sum_{k=1}^{T} \alpha_k}.
\tag{29}</script><p>上式右边：</p>
<script type="math/tex; mode=display">
\frac{R^2 + G^2 \sum_{k=1}^{T} \alpha_k^2}{2 \sum_{k=1}^{T} \alpha_k} 
\geq \frac{2 RG \sqrt{\sum_{k=1}^{T} \alpha_k^2}}{2 \sum_{k=1}^{T} \alpha_k} 
\geq \frac{RG}{\sqrt{T}} \frac{\sqrt{T} \sqrt{\sum_{k=1}^{T} \alpha_k^2}}{\sum_{k=1}^{T} \alpha_k}.
\tag{30}</script><p>通过选择合适的步长$\alpha_k$，满足$T \geq \sum_{k=1}^{T} \alpha_k^2 \left( \sum_{k=1}^{T} \alpha_k \right)^2$，有：</p>
<script type="math/tex; mode=display">
\min_{k \in \{ 1, \cdots, T \}} \epsilon_k \leq \frac{RG}{\sqrt{T}}.
\tag{31}</script><p>假设$C$是simplex，那么$R \leq \sqrt{2}$。如果每个梯度$g_i$的每个坐标都是以$M$为上界，那么$G$最多可以是$M\sqrt{n}$，即取决于维度。</p>
<p>从步骤2到4，可以用$\text{Div}_{\psi}(x^{<em>},x_{k+1})$代替$| x_{k+1} - x^{</em>} |_2^2$。而步骤1，则需要用到引理1。</p>
<p>假设$\psi$是$\sigma$-严格凸函数，将式(23)中$\alpha_k f(x_k) + \alpha_k \left&lt; g_k, x - x_k \right&gt;$的视为引理1中的$L$，那么：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    & \alpha_k f(x_k) + \alpha_k \left< g_k, x^{*} - x_k \right> + \text{Div}_{\psi}(x^{*}, x_k) \\
    \geq & \alpha_k f(x_k) + \alpha_k \left< g_k, x_{k+1} - x_k \right> + \text{Div}_{\psi}(x_{k+1}, x_{k}) + \text{Div}_{\psi}(x^{*}, x_{k+1}).
\end{aligned}
\tag{32}</script><p>移项后得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{Div}_{\psi}(x^{*}, x_{k+1}) \leq & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left< g_k, x^{*} - x_{k+1} \right> - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\overset{式(4)}{=} & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left< g_k, x^{*} - x_{k} \right> + \alpha_k \left< g_k, x_{k} - x_{k+1} \right> - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*})\right) + \alpha_k \left< g_k, x_{k} - x_{k+1} \right> - \frac{\sigma}{2} \| x_{k} - x_{k+1} \|^2 \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*})\right) + \alpha_k \| g_k \|_{*} \| x_{k} - x_{k+1} \| - \frac{\sigma}{2} \| x_{k} - x_{k+1} \|^2 \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*}) \right) + \frac{ \alpha_k^2 }{2 \sigma} \| g_{k} \|^2.
\end{aligned}
\tag{33}</script><p>与式(26)对比，可以将$| x_{k} - x^{<em>} |_2^2$替换为$\text{Div}_{\psi}(x^{</em>}, x_{k})$。同样，假设$\text{Div}_{\psi}(x^{<em>}, x_{1})$界为$R^2$，且$| g_k |_{</em>}$的界为$G$，其中$| \cdot |_{*}$是对偶范数。</p>
<p>为了显示mirror descent的优势，假设$C$是$n$维的simplex，使用kL散度，其中$\psi$是关于$l_1$范数的1-严格凸函数。那么$l_1$范数的对偶范数就是$l_{\infty}$范数。因此，可以用kL散度考虑$\text{Div}_{\psi}(x^{*}, x_{1})$的界为$\log n$，而$G$则上界为$M$。所以，对于$RG$的值，mirror descent比子梯度下降小一个数量级$O(\sqrt{\frac{n}{\log n}})$。</p>
<p><strong>加速1：$f$是强凸函数。</strong>  我们说关于另一个函数$\psi$和模数$\lambda$，$f$是<code>强凸函数</code>，那么满足：</p>
<script type="math/tex; mode=display">
f(x) \geq f(y) + \left< g, x - y \right> + \lambda \text{Div}_{\psi}(x, y) \quad g \in \partial f(y).
\tag{34}</script><p>注意，并不要求$f$是可微的。那么式(33)可以增加强凸的条件：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{Div}_{\psi}(x^{*}, x_{k+1}) \leq & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left< g_k, x^{*} - x_{k+1} \right> - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\overset{式(4)}{=} & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left< g_k, x^{*} - x_{k} \right> + \alpha_k \left< g_k, x_{k} - x_{k+1} \right> - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*}) + \lambda \text{Div}_{\psi}(x^{*}, x_{k}) \right) + \alpha_k \left< g_k, x_{k} - x_{k+1} \right> - \frac{\sigma}{2} \| x_{k} - x_{k+1} \|^2 \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*}) + \lambda \text{Div}_{\psi}(x^{*}, x_{k}) \right) + \alpha_k \| g_k \|_{*} \| x_{k} - x_{k+1} \| - \frac{\sigma}{2} \| x_{k} - x_{k+1} \|^2 \\
\leq & (1 - \lambda \alpha_k ) \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*}) \right) + \frac{ \alpha_k^2 }{2 \sigma} \| g_{k} \|_{*}^2.
\end{aligned}
\tag{35}</script><p>记$\delta_{k} = \text{Div}_{\psi}(x^{*}, x_k)$，令$\alpha_k = \frac{1}{\lambda k}$，那么上式有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \delta_{k+1} \leq \frac{k-1}{k} \delta_{k} - \frac{1}{\lambda k} \epsilon_{k} + \frac{G^2}{2 \sigma \lambda^2 k^2} \\
\Rightarrow & k \delta_{k+1} \leq (k-1) \delta_{k} - \frac{1}{\lambda} \epsilon_{k} + \frac{G^2}{2 \sigma \lambda^2 k}.
\end{aligned}
\tag{36}</script><p>递推求和有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& T \delta_{T+1} \leq - \frac{1}{\lambda} \sum_{k=1}^{T} \epsilon_{k} + \frac{G^2}{2 \sigma \lambda^2} \sum_{k=1}^{T} \frac{1}{k}. \\
\Rightarrow & \min_{i \in \{ 1,\cdot, T \}} \epsilon_{k} \leq \frac{G^2}{2 \sigma \lambda } \frac{1}{T} \sum_{k=1}^{T} \frac{1}{k} \leq
\frac{G^2}{2 \sigma \lambda } \frac{O(\log T)}{T}.
\end{aligned}
\tag{37}</script><p><strong>加速2：$f$的梯度Lipschitz连续。</strong> 如果函数$f$的梯度是<code>Lipschitz连续</code>，那么存在$L &gt; 0$使得：</p>
<script type="math/tex; mode=display">
\| \nabla f(x) - \nabla f(y) \|_{*} \leq L \| x - y \|, \quad \forall x, y.
\tag{38}</script><p>有时候我们直接说$f$是光滑的。上式等价于：</p>
<script type="math/tex; mode=display">
f(x) \leq f(y) + \left< \nabla f(y), x - y \right> + \frac{L}{2} \| x - y \|^2.
\tag{39}</script><p>现在考虑式(33)中的$\left&lt; g_k, x^{*} - x_{k+1} \right&gt;$的界：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left< g_k, x^{*} - x_{k+1} \right> =& \left< g_k, x^{*} - x_{k} \right> + \left< g_k, x_{k} - x_{k+1} \right> \\
\leq & f(x^{*}) - f(x_k) +  \frac{L}{2} \| x^{*} - x_k \|^2 + f(x_{k}) - f(x_{k+1}) + \frac{L}{2} \| x_{k} - x_{k+1} \|^2  \\
= & f(x^{*}) - f(x_{k+1}) + \frac{L}{2} \| x_{k} - x_{k+1} \|^2.
\end{aligned}
\tag{40}</script><p>将其代入式(33)有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{Div}_{\psi}(x^{*}, x_{k+1}) \leq & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left( f(x^{*}) - f(x_{k+1}) + \frac{L}{2} \| x_{k} - x_{k+1} \|^2 \right) - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\overset{式(4)}{=}& \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left( f(x^{*}) - f(x_{k+1}) + \frac{L}{2} \| x_{k} - x_{k+1} \|^2 \right) - \frac{\sigma}{2} \| x_{k} - x_{k+1} \|^2.
\end{aligned}
\tag{41}</script><p>令$\alpha_k = \frac{\sigma}{L}$，有：</p>
<script type="math/tex; mode=display">
\text{Div}_{\psi}(x^{*}, x_{k+1}) \leq \text{Div}_{\psi}(x^{*}, x_k) - \frac{\sigma}{L} \left( f(x_{k+1}) - f(x^{*}) \right).
\tag{42}</script><p>递推之：</p>
<script type="math/tex; mode=display">
\min_{k \in \{2,\cdots, T+1\}} f(x_k) - f(x^{*}) \leq \frac{L \text{Div}_{\psi}(x^{*}, x_1)}{\sigma T} \leq \frac{L R^2}{\sigma T}.
\tag{43}</script><p>这时候的收敛速度为$O(\frac{1}{T})$，如果使用像Nesterov的技术，可以达到$O(\frac{1}{T^2})$。可以使用引理1证明，我们称之为<code>加速近似梯度法(accelerated proximal gradient method, APGM)</code>。</p>
<hr>
<h3 id="2-1-组合目标函数"><a href="#2-1-组合目标函数" class="headerlink" title="2.1 组合目标函数"></a>2.1 组合目标函数</h3><p>假设目标函数为$h(x) = f(x) + r(x)$，其中$f$是光滑的，而$r(x)$是simple的，比如$| x |_1$。如果直接使用上面的优化方式，可以得到收敛速度为$O(\frac{1}{\sqrt{T}})$，因为$h$不是光滑的。我们希望能够获得光滑时的收敛速度$O(\frac{1}{T})$是可以办到的，因为$r(x)$是简单的函数，只需要扩展式(23)如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    x_{k + 1} &= argmin_{x \in C} \left\{ f(x_k) + \left< g_k, x - x_k \right> + r(x) + \frac{1}{\alpha_k} \text{Div}_{\psi}(x, x_k) \right\} \\
    &= argmin_{x \in C} \left\{ \alpha_k f(x_k) + \alpha_k \left< g_k, x - x_k \right> + r(x) + \text{Div}_{\psi}(x, x_k)  \right\} \\
    &= argmin_{x \in C} \left\{ \left< \alpha_k g_k, x \right> + r(x) + \text{Div}_{\psi}(x, x_k)  \right\}.
\end{aligned}
\tag{44}</script><p>这里我们只采用了$f$在$x_k$附近的一阶近似，不考虑$r(x)$。假设这个近似操作可以有效地计算，那么我们就可以证明上述的速率都能延续。</p>
<p>这里考虑更为一般的情况，即$f$是非光滑或者强凸的，如果$f$的梯度是Lipschitz连续，依然能够获得$O(\frac{1}{T^2})$的收敛速度。</p>
<p>将$\alpha_k f(x_k) + \alpha_k \left&lt; g_k, x - x_k \right&gt; + r(x)$视作引理1中的$L$，那么有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \alpha_k f(x_k) + \alpha_k \left< g_k, x^{*} - x_k \right> + r(x^{*}) + \text{Div}_{\psi}(x^{*}, x_k) \\
\geq & \alpha_k f(x_k) + \alpha_k \left< g_k, x_{k+1} - x_k \right> + r(x_{k+1}) + \text{Div}_{\psi}(x_{k+1}, x_k) + \text{Div}_{\psi}(x^{*}, x_{k+1}).
\end{aligned}
\tag{45}</script><p>那么，同式(33),有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{Div}_{\psi}(x^{*}, x_{k+1}) \leq & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left< g_k, x^{*} - x_{k+1} \right> + \alpha_k \left( r(x^{*}) - r(x_{k+1}) \right) - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\leq & \cdots \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) + r(x_{k+1}) - f(x^{*}) - f(r(x^{*}) ) \right) + \frac{ \alpha_k^2 }{2 \sigma} \| g_{k} \|^2.
\end{aligned}
\tag{46}</script><p>记$\delta_k = \text{Div}_{\psi}(x^{*}, x_k)$，那么：</p>
<script type="math/tex; mode=display">
f(x_{k}) + r(x_{k+1}) - f(x^{*}) - f(r(x^{*}) \leq \frac{1}{\alpha_k} ( \delta_k - \delta_{k+1} ) + \frac{ \alpha_k }{2 \sigma} \| g_{k} \|_{*}^2.
\tag{47}</script><p>累加求和得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& r(x_{T+1}) - r(x_1) + \sum_{k=1}^{T} \left( h(x_k) - h(x^{*}) \right) \\
\leq & \frac{\delta_1}{\alpha_1} + \sum_{k=2}^{T} \delta_k \left( \frac{1}{\alpha_k} - \frac{1}{\alpha_{k-1}} \right) - \frac{\delta_{T+1}}{\alpha_T} + \frac{ G^2 }{2 \sigma} \sum_{k=1}^{T} \alpha_k \\
\leq & R^2 \left( \frac{1}{\alpha_1} + \sum_{k=2}^{T} \left( \frac{1}{\alpha_k} - \frac{1}{\alpha_{k-1}} \right) \right) + \frac{ G^2 }{2 \sigma} \sum_{k=1}^{T} \alpha_k \\
= & \frac{R^2}{\alpha_T} + \frac{ G^2 }{2 \sigma} \sum_{k=1}^{T} \alpha_k.
\end{aligned}
\tag{48}</script><p>假设选择$x_1 = argmin_{x} r(X)$，那么有$r(x_{T+1}) - r(x_1) \geq 0$。令$\alpha_k = \frac{R}{G} \sqrt{\frac{\sigma}{k}}$，则有：</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{T} \left( h(x_k) - h(x^{*}) \right) \leq \frac{RG}{\sqrt{\sigma}} \left( \sqrt(T) + \frac{1}{2} \sum_{k=1}^{T} \frac{1}{\sqrt{k}} \right) = \frac{RG}{\sqrt{\sigma}} O(\sqrt{T}).
\tag{49}</script><p>因此$\min_{k=1,\cdots,T} \{ h(x_k) - h(x^{*}) \}$的收敛速度为$O(\frac{RG}{\sqrt{\sigma}T})$。</p>
<hr>
<h3 id="2-2-在线学习"><a href="#2-2-在线学习" class="headerlink" title="2.2 在线学习"></a>2.2 在线学习</h3><p><img src="Protocol_of_online_learning_1.jpg" alt=""></p>
<p>上图是在线学习算法。玩家在线学习的目的就是尽量减少遗憾，使得可能的损失$\sum_{k}f_k (x)$在所有可能的$x$上最小：</p>
<script type="math/tex; mode=display">
\text{Regret} = \sum_{k=1}^Tf_k (x_k) - \min_{x} \sum_{k=1}^{T} f_k (x).
\tag{50}</script><p>需要注意的是，没有假设对手如何选$f_k$，可以对立。在第$k$次迭代获得$f_k$后，使用mirror descent对$f_k$进行更新获得$x_{k+1}$：</p>
<script type="math/tex; mode=display">
x_{k+1} = argmin_{x \in C} \left\{ f_k(x_k) + \left< g_k, x - x_k \right> + \frac{1}{\alpha_k} \text{Div}(x, x_k) \right\}, \quad g_k \in \partial f_x (x_k).
\tag{51}</script><p>可以推得regret有界。由式(33)有：</p>
<script type="math/tex; mode=display">
f_k(x_k) - f_k(x^{*}) \leq \frac{1}{\alpha_k} \left( \text{Div}(x^{*}, x_k) - \text{Div}(x^{*}, x_{k+1}) \right) + \frac{\alpha_k}{2 \sigma} \| g_k \|_{*}^{2}.
\tag{52}</script><p>从$k=1$开始累加，同式(48)和式(49)：</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{T} \left( f_k(x_k) - f_k(x^{*}) \right)\leq \frac{RG}{\sqrt{\sigma}} O(\sqrt{T}).
\tag{53}</script><p>所以，regret的是以$O(\sqrt{T})$的速度增长。</p>
<p><strong>$f$是强凸函数</strong> 将$f_k$代替式(35)中的$f$，得到regret的界为$O(\log T)$。</p>
<p><strong>$f$的梯度是Lipschitz连续</strong> 式(43)的结果无法推广到在线学习的情况，如果用$f_k$代替$f$，那么会得到等式右边有$f_k(x_{k+1}) - f_k (x^{*})$。递推求和无法得到regret的界。因此，梯度的Lipschitz连续无法保证regret的界为$O(\log T)$。</p>
<p><strong>组合目标函数</strong> 在在线设定中，玩家和对手都有$r(x)$，对手在每次迭代时改变$f_k(x)$。每次迭代的目标函数是$h_k(x_k) = f_k(x_k) + r(x_k)$。更新规则为：</p>
<script type="math/tex; mode=display">
x_{k+1} = argmin_{x \in C} \left\{ f_k(x_k) + \left< g_k, x - x_k \right>  + r(x) + \frac{1}{\alpha_k} \text{Div}(x, x_k) \right\}, \quad g_k \in \partial f_x (x_k).
\tag{54}</script><p>那么式(47)变为：</p>
<script type="math/tex; mode=display">
f_k(x_{k}) + r(x_{k+1}) - f_k(x^{*}) - f_k(r(x^{*}) \leq \frac{1}{\alpha_k} ( \delta_k - \delta_{k+1} ) + \frac{ \alpha_k }{2 \sigma} \| g_{k} \|_{*}^2.
\tag{55}</script><p>虽然我们这里用的是$r(x_{k+1})$，而不是$r(x_k)$，但这是没有问题的，因为$r$不会通过迭代而改变。<br>选择$x_1 = argmin_{x} r(x)$，同式(48,49)有：</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{T} \left( h_k(x_k) - h_k(x^{*}) \right) \leq \frac{G}{\sqrt{\sigma}} O(\sqrt{T}).
\tag{56}</script><p>因此，有$O(\sqrt{T})$。</p>
<p>当$f_k$为强凸时，我们可以得到组合情况下的$O(\log T)$regret。但是，同上面一样，$f$梯度的Lipschitz连续无法保证regret的界为$O(\log T)$。</p>
<hr>
<h3 id="2-3-随机优化"><a href="#2-3-随机优化" class="headerlink" title="2.3 随机优化"></a>2.3 随机优化</h3><p>让我们考虑优化一个函数，它的期望值形式为</p>
<script type="math/tex; mode=display">
\min_{x} F(x) := \mathbb{E}_{w \sim p} [ f(x; w) ],
\tag{57}</script><p>其中$p$是$w$的分布。这其中包含了很多机器学习模型。比如SVM的目标函数为：</p>
<script type="math/tex; mode=display">
F(x) = \frac{1}{m} \sum_{i=1}^{m} \max \left\{ 0, 1 - c_i \left< a_i, x \right> + \frac{\lambda}{2} \| x \|^2 \right\}.
\tag{58}</script><p>它可以解释成式(57)，$w$是在$\{ 1, 2, \cdots, m \}$上的均匀分布，例如$p(w=i) = \frac{1}{m}$：</p>
<script type="math/tex; mode=display">
f(x; i) = \max \left\{ 0, 1 - c_i \left< a_i, x \right> \right\} + \frac{\lambda}{2} \| x \|^2.
\tag{59}</script><p>当$m$较大时，计算$F$及其子梯度的成本会很高。所以，一个简单的想法是基于一个随机选择的数据点进行更新。它可以认为是算法1中在线学习的一种特殊情况，步骤4中的对手现在随机选取$f_k$为$f(x;w_k)$，$w_k$与$p$无关。理想情况下，我们希望通过使用mirror descent更新，$x_k$将逐渐接近$F(x)$的最小化值。直观上这是非常合理的，通过使用$f_k$，我们可以计算出$F(x_k)$的无偏估计值和$F(x_k)$的次梯度(因为$w_k$是从$p$中进行独立同分布采样得到的)。这是随机优化的一种特殊情况，我们在算法2中进行了总结。</p>
<p><img src="Protocol_of_online_learning_2.jpg" alt=""></p>
<p>事实上，该方法在更一般的环境下也是有效的。为了简单起见，我们只说对手在迭代$k$时有$w_k$。那么在线学习算法$\mathcal{A}$就是简单的从一个有序集$\{ w_1, \cdots, w_k \}$到$x_{k+1}$的确定性映射。将初始模型$x_1$表示为$\mathcal{A}(w_0)$。那么下面的定理就是在线到批量转换的关键。</p>
<p><strong>定理1</strong> 假设在线学习算法$\mathcal{A}$在使用算法1迭代$k$次后regret有上界$R_k$。假设$w_1, \cdots, w_{T+1}$独立同分布采样自$p$。定义$\hat{x}=\mathcal{A}(w_{j+1}, \cdots, w_{T})$，其中$j$均匀随机采样自$\{ 0, \cdots, T \}$。那么：</p>
<script type="math/tex; mode=display">
\mathbb{E}[F(\hat{x})] - \min_{x} F(x) \leq \frac{R_{T+1}}{T + 1}.
\tag{60}</script><p>其中期望值与$w_1, \cdots, w_{T}$和$j$的随机性有关。</p>
<p>相似地，可以用$1 - \sigma$高概率保持边界：</p>
<script type="math/tex; mode=display">
F(\hat{x}) - \min_{x} F(x) \leq \frac{R_{T+1}}{T + 1} \log \frac{1}{\sigma}.
\tag{61}</script><p>其中概率与$w_1, \cdots, w_{T}$和$j$的随机性有关。</p>
<p><em>证明</em> </p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E} [F(\hat{x})] =& \mathbb{E}_{j,w_1, \cdots, w_{T+1}} [f(\hat{x}; w_{T+1})] \\
=& \mathbb{E}_{j,w_1, \cdots, w_{T+1}} [f(\mathcal{A}_{w_{j+1}, \cdots, w_{T}}; w_{T+1})] \\
=& \mathbb{E}_{w_1, \cdots, w_{T+1}} \left[ \frac{1}{T + 1} \sum_{j=0}^{T} f(\mathcal{A}_{w_{j+1}, \cdots, w_{T}}; w_{T+1}) \right] \quad (j \text{是均匀随机抽取的}) \\
=& \frac{1}{T + 1} \mathbb{E}_{w_1, \cdots, w_{T+1}} \left[ \sum_{j=0}^{T} f(\mathcal{A}_{w_{1}, \cdots, w_{T-j}}; w_{T+1-j}) \right] \quad (\text{平移指标}) \\
=& \frac{1}{T + 1} \mathbb{E}_{w_1, \cdots, w_{T+1}} \left[ \sum_{s=1}^{T+1} f(\mathcal{A}_{w_{1}, \cdots, w_{s-1}}; w_{s}) \right] \quad (\text{替换变量} s = T+1-j ) \\
\leq & \frac{1}{T + 1} \mathbb{E}_{w_1, \cdots, w_{T+1}} \left[ \min_{x} \sum_{s=1}^{T+1} f(x; w_{s}) + R_{T+1} \right] \quad (\text{利用regret的界}) \\
\leq & \min_{x} \mathbb{E}_{w} [f(x;w)] + \frac{R_{T+1}}{T+1} \quad (\text{最小值的期望值小于期望值的最小值}) \\ 
= & \min_{x} F(x) + \frac{R_{T+1}}{T + 1}.
\end{aligned}
\tag{62}</script>]]></content>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
</search>
