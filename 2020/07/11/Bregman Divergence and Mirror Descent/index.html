<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Bregman Divergence and Mirror Descent [1] Bregman Divergence and Mirror Descent   I. Bregman Divergence动机  将欧几里得距离的平方概括为一类距离，这些距离都具有相似的性质。 在机器学习、聚类、指数族等方面有很多应用。  定义1（Bregman divergence） 函数$\psi : \Ome">
<meta property="og:type" content="article">
<meta property="og:title" content="Bregman Divergence and Mirror Descent">
<meta property="og:url" content="http://yoursite.com/2020/07/11/Bregman%20Divergence%20and%20Mirror%20Descent/index.html">
<meta property="og:site_name" content="青山白云间的博客">
<meta property="og:description" content="Bregman Divergence and Mirror Descent [1] Bregman Divergence and Mirror Descent   I. Bregman Divergence动机  将欧几里得距离的平方概括为一类距离，这些距离都具有相似的性质。 在机器学习、聚类、指数族等方面有很多应用。  定义1（Bregman divergence） 函数$\psi : \Ome">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/07/11/Bregman%20Divergence%20and%20Mirror%20Descent/Protocol_of_online_learning_1.jpg">
<meta property="og:image" content="http://yoursite.com/2020/07/11/Bregman%20Divergence%20and%20Mirror%20Descent/Protocol_of_online_learning_2.jpg">
<meta property="article:published_time" content="2020-07-11T08:51:12.152Z">
<meta property="article:modified_time" content="2020-07-11T08:59:44.886Z">
<meta property="article:author" content="青山白云间">
<meta property="article:tag" content="优化算法">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/07/11/Bregman%20Divergence%20and%20Mirror%20Descent/Protocol_of_online_learning_1.jpg">

<link rel="canonical" href="http://yoursite.com/2020/07/11/Bregman%20Divergence%20and%20Mirror%20Descent/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Bregman Divergence and Mirror Descent | 青山白云间的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">青山白云间的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/11/Bregman%20Divergence%20and%20Mirror%20Descent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="青山白云间">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="青山白云间的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Bregman Divergence and Mirror Descent
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-07-11 16:51:12 / 修改时间：16:59:44" itemprop="dateCreated datePublished" datetime="2020-07-11T16:51:12+08:00">2020-07-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Bregman-Divergence-and-Mirror-Descent"><a href="#Bregman-Divergence-and-Mirror-Descent" class="headerlink" title="Bregman Divergence and Mirror Descent"></a>Bregman Divergence and Mirror Descent</h1><blockquote>
<p>[1] <a href="http://users.cecs.anu.edu.au/~xzhang/teaching/bregman.pdf" target="_blank" rel="noopener">Bregman Divergence and Mirror Descent</a></p>
</blockquote>
<hr>
<h2 id="I-Bregman-Divergence"><a href="#I-Bregman-Divergence" class="headerlink" title="I. Bregman Divergence"></a>I. Bregman Divergence</h2><p><strong>动机</strong></p>
<ul>
<li>将欧几里得距离的平方概括为一类距离，这些距离都具有相似的性质。</li>
<li>在机器学习、聚类、指数族等方面有很多应用。</li>
</ul>
<p><strong>定义1（Bregman divergence）</strong> 函数$\psi : \Omega \rightarrow \mathbb{R}$满足：<br>a). 严凸<br>b). 连续可微<br>c). 定义在一个封闭的凸集$\Omega$上。<br>那么<code>Bregman散度</code>可以定义为：</p>
<script type="math/tex; mode=display">
\text{Div}_{\psi} (x, y) = \psi(x) - \psi(y) - \left< \nabla \psi (y), x - y \right>.
\tag{1}</script><p>即为$\psi$在$x$处的值与$\psi$在$y$周围的一阶泰勒展开式取$x$的值之间的差。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>函数名</th>
<th>$\psi (x)$</th>
<th>函数定义域</th>
<th>$\text{Div}_{\psi} (x, y)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Squared norm</td>
<td>$\frac{1}{2}x^2$</td>
<td>$(-\infty, +\infty)$</td>
<td>$\frac{1}{2}(x-y)^2$</td>
</tr>
<tr>
<td>Shannon entropy</td>
<td>$x \log x - x$</td>
<td>$[0, +\infty)$</td>
<td>$x \log \frac{x}{y} - x +y$</td>
</tr>
<tr>
<td>Bit entropy</td>
<td>$x \log x  + (1 - x) \log (1 - x)$</td>
<td>$[0, 1]$</td>
<td>$x \log \frac{x}{y} + (1 - x) \log \frac{(1 - x)}{(1 - y)}$</td>
</tr>
<tr>
<td>Burg entropy</td>
<td>$- \log x$</td>
<td>$(0, +\infty)$</td>
<td>$\frac{x}{y} - \log \frac{x}{y} - 1$</td>
</tr>
<tr>
<td>Hellinger</td>
<td>$- \sqrt{1 - x^2}$</td>
<td>$[-1, 1]$</td>
<td>$(1 - xy)(1 - y^2)^{-\frac{1}{2}} - (1 - x^2 )^{\frac{1}{2}}$</td>
</tr>
<tr>
<td>$l_p$ quasi-norm</td>
<td>$- x^p \quad (0&lt;p&lt;1)$</td>
<td>$[0, +\infty)$</td>
<td>$-x^p+pxy^{p-1}-(p-1)y^p$</td>
</tr>
<tr>
<td>$l_p$ norm</td>
<td>$- \vert x \vert^p \quad (1&lt;p&lt;\infty)$</td>
<td>$(-\infty, +\infty)$</td>
<td>$\vert x \vert^p - p x y^{p-1} \text{sgn}(y) + (p-1) \vert y \vert^p$</td>
</tr>
<tr>
<td>Exponential</td>
<td>$\exp(x)$</td>
<td>$(-\infty, +\infty)$</td>
<td>$\exp(x) - \left(x - y + 1 \right) \exp(y)$</td>
</tr>
<tr>
<td>Hellinger</td>
<td>$\frac{1}{x}$</td>
<td>$(0, +\infty)$</td>
<td>$\frac{1}{x} + \frac{x}{y^2} - \frac{2}{y}$</td>
</tr>
</tbody>
</table>
</div>
<p><strong>例子</strong></p>
<ul>
<li>欧式距离。令$\psi(x)=\frac{1}{2}|x|^2$，则$\text{Div}_{\psi}(x,y) = \frac{1}{2} | x - y |^2$。</li>
<li>$\Omega = \{ x \in \mathbb{R}_{+}^{n} : \sum_{i} x_i = 1 \}$，且$\psi(x) = \sum_{i} x_i \log x_i$。那么$\text{Div}_{\psi}(x,y) = \sum_{i} x_i \log \frac{x_i}{y_i}, x, y \in \Omega$。这就是所谓的相对熵，或概率分布$x$和$y$之间的Kullback-Leibler散度。</li>
<li>$l_p$范式。令$p \geq 1$且$\frac{1}{p} + \frac{1}{q} = 1$，$\psi(x)=\frac{1}{2}|x|_q^2$。那么$\text{Div}_{\psi}(x,y) = \frac{1}{2}|x|_q^2 + \frac{1}{2}|y|_q^2 - \left&lt; x , \nabla \frac{1}{2}|y|_q^2 \right&gt;$。注意$\frac{1}{2}|y|_q^2$不一定是可连续微分的，这就使得这种情况与我们的定义不完全一致。</li>
</ul>
<p><strong>Properties of Bregman divergence</strong></p>
<p><code>1.</code> Bregman散度$\text{Div}_{\psi}(x,y)$关于第一个变量$x$是严格凸函数，那么函数$\psi$的严格凸性并不重要。</p>
<blockquote>
<p><em>原文：</em> Strict convexity in the first argument $x$. Trivial by the strict convexity of $\psi$.</p>
</blockquote>
<p><code>2.</code> 非负性。$\forall x, y$，有$\text{Div}_{\psi}(x,y) \geq 0$，等号当且仅当$x=y$时取得。不要求严格凸亦成立（Trivial by strict convexity.）。</p>
<p><code>3.</code> 非对称性。一般情况下$\text{Div}_{\psi}(x,y) \neq \text{Div}_{\psi}(y,x)$，例如KL散度。对称性并不是一直有用。</p>
<p><code>4.</code> 关于第二变量$y$非凸。令$\Omega = [1, \infty), \psi(x) = - \log x$。那么$\text{Div}_{\psi}(x,y) = - \log x + \log y + \frac{x-y}{y}$。计算其关于$y$的二阶偏导为$\frac{1}{y^2}(\frac{2x}{y}-1)$，当$2x &lt; y$时为负。</p>
<p><code>5.</code> 关于$\psi$的线性。对于任意$a &gt; 0$，有$\text{Div}_{\psi + a \phi}(x,y)= \text{Div}_{\psi}(x,y) + a \text{Div}_{\phi}(x,y)$。</p>
<p><code>6.</code> 关于$x$的梯度。$\frac{\partial }{ \partial x} \text{Div}_{\psi}(x,y) = \nabla \psi (x) - \nabla \psi (y)$。关于$y$的梯度比较棘手，不常用。</p>
<p><code>7.</code> 广义三角形不等式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \text{Div}_{\psi}(x,y) + \text{Div}_{\psi}(y,z) &= \psi(x) - \psi(y) - \left< \nabla \psi (y), x - y \right> + \psi(y) - \psi(z) - \left< \nabla \psi (z), y - z \right> \\
    &= \text{Div}_{\psi}(x,z) + \left< x - y, \nabla \psi (z) - \nabla \psi (y) \right>.
\end{aligned}
\tag{2}</script><p><code>8.</code> 特例，如果$\psi$对于某些范式和模数$\sigma$满足：</p>
<script type="math/tex; mode=display">
\psi (x) \geq \psi(y) + \left< \nabla \psi(y), x - y\right> + \frac{\sigma}{2} \| x - y \|^2.
\tag{3}</script><p>则称$\psi$是$\sigma$-<code>强凸函数</code>。需要注意的是，并不需要是欧几里得范式。如果是欧式范式，那么上式条件等价于$\psi(x) - \frac{\sigma}{2} | x |^2$是凸函数。比如，KL散度中的$\psi (x) = \sum_{i} x_i$是在$\Omega = \{ x \in \mathbb{R}_{+}^{n} : \sum_{i} x_i = 1 \}$上$l_1$范式下的1-强凸函数。当$\psi$是$\sigma$-强凸函数，有：</p>
<script type="math/tex; mode=display">
\text{Div}_{\psi}(x,y) \geq \frac{\sigma}{2} \| x - y \|^2.
\tag{4}</script><p><em>证明：</em> 根据定义有$\text{Div}_{\psi} (x, y) = \psi(x) - \psi(y) - \left&lt; \nabla \psi (y), x - y \right&gt; \geq \frac{\sigma}{2} | x - y |^2$。</p>
<p><code>9.</code> 对偶性。假设$\psi$是强凸函数，那么</p>
<script type="math/tex; mode=display">
(\nabla \psi^{*}) \nabla \psi (x) = x, \qquad \text{Div}_{\psi} (x, y) = \text{Div}_{\psi^{*}} (\nabla \psi (y), \nabla \psi (x)).
\tag{5}</script><p><em>证明：</em> 先证第一个等式。根据：</p>
<script type="math/tex; mode=display">
\psi^{*}(y) = \sup_{z \in \Omega} \{ \left< z, y \right> - \psi (z) \}.
\tag{6}</script><p>$\sup$是可以实现的，因为$\psi$是强凸的，且定义域$\Omega$是闭集。如果在$x$处取得最大值，当且仅当$y = \nabla \psi (x)$（令$\frac{\partial}{\partial z} \left( \left&lt; z, y \right&gt; - \psi (z)  \right) = y - \nabla \psi (z) =0$即得）。因此</p>
<script type="math/tex; mode=display">
\psi^{*}(y) + \psi (y) = \left< x, y \right> \quad \Leftrightarrow \quad y = \nabla \psi (x).
\tag{7}</script><p>因为$\psi = \psi^{<strong>}$，因此$\psi^{*}(y) + \psi^{</strong>} (y) = \left&lt; x, y \right&gt;$，也就是说</p>
<script type="math/tex; mode=display">
\psi^{**}(y) = \sup_{z \in \Omega} \{ \left< z, y \right> - \psi^{*} (z) \}
\tag{8}</script><p>在$y$处取得最大值，即$x = \nabla^{<em>} \psi (y)$。因此$(\nabla \psi^{</em>}) (\nabla \psi (x) ) = \nabla \psi^{*} (y) = x$，证毕。</p>
<p>再证第二等式。根据式(6)有$\psi^{<em>}( \nabla \psi (y) ) = \sup_{z \in \Omega} \{ \left&lt; z, \nabla \psi (y) \right&gt; - \psi (z) \}$，同样的，令$\frac{\partial}{\partial z} \left( \left&lt; z, \nabla \psi (y) \right&gt; - \psi (z) \right) =0$，便得到$\psi^{</em>}( \nabla \psi (y) ) = \left&lt; y, \nabla \psi (y) \right&gt; - \psi (y)$。</p>
<p>同理有$\psi^{*}( \nabla \psi (x) ) = \left&lt; x, \nabla \psi (x) \right&gt; - \psi (x)$。那么</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \text{Div}_{\psi^{*}} (\nabla \psi (y), \nabla \psi (x)) &= \psi^{*}(\nabla \psi (y)) - \psi^{*}(\nabla \psi (x)) - \left< \nabla \psi^{*} (\nabla \psi (x)), \nabla \psi (y) - \nabla \psi (x) \right> \\
    & = \left< y, \nabla \psi (y) \right> - \psi (y) - \left< x, \nabla \psi (x) \right> + \psi (x) - \left< x, \nabla \psi (y) - \nabla \psi (x) \right> \\
    &= \left< y, \nabla \psi (y) \right> - \psi (y) - \left< x, \nabla \psi (x) \right> + \psi (x) - \left< x, \nabla \psi (y) \right>  + \left< x, \nabla \psi (x) \right> \\
    &= \psi (x) - \psi (y) - \left< x - y, \nabla \psi (y) \right>\\
    &= \text{Div}_{\psi} (x, y).
\end{aligned}
\tag{9}</script><p><code>10.</code> 分布的平均值。假设$U$是开集$S$服从$\mu$分布的随机变量，那么</p>
<script type="math/tex; mode=display">
\min_{x \in S} \mathbb{E}_{U \sim \mu} \left[ \text{Div}_{\psi} (U, x) \right].
\tag{10}</script><p>在$\bar{\mu} := \mathbb{E}_{\mu} = \int_{u \in S} u \mu(u)$处取得最小值。</p>
<p><em>证明：</em> 对于任意的$x \in S$，都有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    & \mathbb{E}_{U \sim \mu} \left[ \text{Div}_{\psi} (U, x) \right] - \mathbb{E}_{U \sim \mu} \left[ \text{Div}_{\psi} (U, \bar{\mu}) \right] \\
    =& \mathbb{E}_{U \sim \mu} \left[ \psi(U) - \psi(x) - \left< \nabla \psi (x), U - x \right> - \psi(U) + \psi(\bar{\mu}) + \left< \nabla \psi (\bar{\mu}), U - \bar{\mu} \right> \right] \\
    =& \psi(\bar{\mu}) - \psi(x) + x^T \nabla \psi (x) - \bar{\mu}^T \nabla \psi (\bar{\mu}) + \mathbb{E}_{U \sim \mu} \left[ - U^T \nabla \psi (x) + U^T \nabla \psi (\bar{\mu}) \right] \\
    =& \psi(\bar{\mu}) - \psi(x) - (\bar{\mu} - x)^T \nabla \psi (x) + \bar{\mu}^T \nabla \psi (x)- \bar{\mu}^T \nabla \psi (\bar{\mu}) \\
    &- (\mathbb{E}_{U \sim \mu} \left[ U \right])^T \nabla \psi (x) + (\mathbb{E}_{U \sim \mu} \left[ U \right])^T \nabla \psi (\bar{\mu})\\
    =& \psi(\bar{\mu}) - \psi(x) - (\bar{\mu} - x)^T \nabla \psi (x) \\
    =& \text{Div}_{\psi} (\bar{\mu}, x).
\end{aligned}
\tag{11}</script><p>上式是非负的，当且仅当$x = \bar{\mu}$时为0。</p>
<p><code>11.</code> 毕达哥拉斯定理（三点定理）。如果$x^{*}$是$x_0$在凸集$C \in \Omega$的投影：</p>
<script type="math/tex; mode=display">
x^{*} = argmin_{x \in C} \text{Div}_{\psi} (x, x_0).
\tag{12}</script><p>那么</p>
<script type="math/tex; mode=display">
\text{Div}_{\psi} (y, x_0) \geq \text{Div}_{\psi} (y, x^{*}) + \text{Div}_{\psi} (x^{*}, x_0).
\tag{13}</script><p>如果是欧式的情况下，上式的意味着$\angle y x^{*} x_0$是钝角。更为一般的情况，为引理1。</p>
<p><strong>引理1：</strong> 假设$L$是一个合适的函数(proper convex function)，其定义域是一个包含$C$的开集。$L$不一定是可微的。令$x^{*}$为：</p>
<script type="math/tex; mode=display">
x^{*} = argmin_{x \in C} \{ L(x) + \text{Div}_{\psi} (x, x_0) \}.
\tag{14}</script><p>那么对于任意的$y \in C$,有</p>
<script type="math/tex; mode=display">
L(y) + \text{Div}_{\psi} (y, x_0) \geq L(x^{*}) + \text{Div}_{\psi} (x^{*}, x_0) + \text{Div}_{\psi} (y, x^{*}).
\tag{15}</script><p>式(12)是$L=0$的特殊情况。这个性质是分析Bregman散度相关优化算法的关键。</p>
<p><em>证明：</em> 记$J(x) = L(x) + \text{Div}_{\psi} (x, x_0)$，因为$x^{<em>}$是$J$在$C$上的极小值点，因此存在<code>次梯度(subgradient)</code>$d \in \partial J(x^{</em>})$（<em>这里使用次梯度是因为$L$不一定可微</em>），使得</p>
<script type="math/tex; mode=display">
\left< d, x - x^{*} \right> \geq 0, \quad x \in C.
\tag{16}</script><p>因为</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \partial J(x^{*}) &= \{ g + \nabla_{x = x^{*}} \text{Div}_{\psi} (x, x_0) : g \in \partial L(x^{*})\}  \\
    &= \{ g + \nabla \psi (x^{*}) - \nabla \psi (x_0) : g \in \partial L(x^{*})\}.
\end{aligned}
\tag{17}</script><p>因此，必然存在一个次梯度$g \in \partial L(x^{*})$使得：</p>
<script type="math/tex; mode=display">
\left< g + \nabla \psi (x^{*}) - \nabla \psi (x_0), x - x^{*} \right> \geq 0, \quad x \in C.
\tag{18}</script><p>对于任意的$y \in C$，根据次梯度的性质有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    L(y) \overset{次梯度性质}{\geq}& L(x^{*}) + \left< g, y - x^{*} \right> \\
    \overset{式(18)}{\geq}& L(x^{*}) + \left< \nabla \psi (x^{*}) - \nabla \psi (x_0), y - x^{*} \right> \\
    \overset{分凑项}{=}& L(x^{*}) - \left< \nabla \psi (x_0), x^{*} - x_0 \right> + \psi (x^{*}) - \psi (x_0) \\
    &+ \left< \nabla \psi (x_0), y - x_0 \right> - \psi (y) + \psi (x_0) \\
    &- \left< \nabla \psi (x^{*}), y - x^{*} \right> + \psi (y) - \psi (x^{*}) \\
    =& L(x^{*}) + \text{Div}_{\psi} (x^{*}, x_0) - \text{Div}_{\psi} (y, x_0) + \text{Div}_{\psi} (y, x^{*}).
\end{aligned}
\tag{19}</script><p>将$\text{Div}_{\psi} (y, x_0)$移动到不等式左边，完成证明。</p>
<hr>
<h2 id="II-Mirror-Descent"><a href="#II-Mirror-Descent" class="headerlink" title="II. Mirror Descent"></a>II. Mirror Descent</h2><p>次梯度下降的收敛速度通常取决于问题的维数。假设求函数$f$在$C$上的最小值，那么<code>次梯度下降(subgradient descent)</code>为</p>
<script type="math/tex; mode=display">
\begin{aligned}
    x_{k+\frac{1}{2}} &= x_k - \alpha_k g_k, \quad g_k \in \partial f(x_k) \\
    x_{k+1} &= argmin_{x \in C} \frac{1}{2} \| x - x_{k+\frac{1}{2}} \|^2 = argmin_{x \in C} \frac{1}{2} \| x - \left( x_k - \alpha_k g_k \right) \|^2.
\end{aligned}
\tag{20}</script><p>可以解释如下。用$f$在$x_k$附近的一阶Taylor展开式近似$f$：</p>
<script type="math/tex; mode=display">
f(x) \approx f(x_k) + \left< g_k, x - x_k \right>. 
\tag{21}</script><p>然后用$\frac{1}{2 \alpha_k} | x - x_k |^2$余项作为惩罚项。因此，更新规则是找到下式的极小值：</p>
<script type="math/tex; mode=display">
x_{k+1} = argmin_{x \in C} \left\{ f(x_k) + \left< g_k, x - x_k \right> + \frac{1}{2 \alpha_k} \| x - x_k \|^2 \right\}.
\tag{22}</script><p>上式(22)与式(20)等价。为了将方法推广到欧几里得距离以外，可以直接使用Bregman散度作为余项的量度：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    x_{k + 1} &= argmin_{x \in C} \left\{ f(x_k) + \left< g_k, x - x_k \right> + \frac{1}{\alpha_k} \text{Div}_{\psi}(x, x_k) \right\} \\
    &= argmin_{x \in C} \left\{ \alpha_k f(x_k) + \alpha_k \left< g_k, x - x_k \right> + \text{Div}_{\psi}(x, x_k)  \right\} \\
    &= argmin_{x \in C} \left\{ \left< \alpha_k g_k, x \right> + \text{Div}_{\psi}(x, x_k)  \right\}.
\end{aligned}
\tag{23}</script><p><strong>镜像梯度的解释</strong> </p>
<p>假设约束集$C$是整个空间（即无约束）。那么我们可以用关于$x$的梯度，寻找最优条件：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    &  \frac{\partial}{\partial x} \left( \left< g_k, x \right> + \frac{1}{\alpha_k} \text{Div}_{\psi}(x, x_k)  \right) |_{x = x_{k+1}} = g_k + \frac{1}{\alpha} \left( \nabla \psi(x_{k+1}) - \nabla \psi(x_{k}) \right) = 0 \\
    \Leftrightarrow & \nabla \psi(x_{k+1}) = \nabla \psi(x_{k}) - \left< \alpha_k g_k, x \right> \\
    \Leftrightarrow & x_{k+1} = \left( \nabla \psi \right)^{-1} \left( \nabla \psi(x_{k}) - \left< \alpha_k g_k, x \right> \right) = \left( \nabla \psi^{*} \right) \left( \nabla \psi(x_{k}) - \left< \alpha_k g_k, x \right> \right).
\end{aligned}
\tag{24}</script><p>如果是KL散度，那么$\nabla_{x_k(i)} \psi(x_{k}) = \log x_k(i) + 1$，更新规则为：</p>
<script type="math/tex; mode=display">
x_{k+1}(i) = x_{k}(i) \exp \left( - \alpha_k g_k(i) \right).
\tag{25}</script><p><strong>收敛速度</strong> </p>
<p>回顾在无约束的子梯度下降中，4个步骤：</p>
<p><code>1.</code> 受单次更新的约束:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\| x_{k+1} - x^{*} \|_2^2 =& \| x_{k} - \alpha_k g_k - x^{*} \|_2^2 \\
=& \| x_{k} - x^{*} \|_2^2 - 2 \alpha_k \left< g_k, x_k - x^{*} \right> + \alpha_k^2 \| g_k \|_2^2 \\
\leq & \| x_{k} - x^{*} \|_2^2 - 2 \alpha_k \left( f(x_k) - f(x^{*}) \right) + \alpha_k^2 \| g_k \|_2^2.
\end{aligned}
\tag{26}</script><p>上式用到了$f(x^{<em>}) \geq f(x_k) + \left&lt; g_k, x^{</em>} - x_k \right&gt;$。</p>
<p><code>2.</code> 递推求和：</p>
<script type="math/tex; mode=display">
\| x_{T+1} - x^{*} \|_2^2 \leq \| x_{1} - x^{*} \|_2^2 - 2 \sum_{k=1}^{T} \alpha_k \left( f(x_k) - f(x^{*}) \right) + \sum_{k=1}^{T} \alpha_k^2 \| g_k \|_2^2.
\tag{27}</script><p><code>3.</code> 根据$| x_{1} - x^{*} |_2^2 \leq R^2$和$| g_k |_2^2 \leq G^2$：</p>
<script type="math/tex; mode=display">
2 \sum_{k=1}^{T} \alpha_k \left( f(x_k) - f(x^{*}) \right) \leq R^2 + G^2 \sum_{k=1}^{T} \alpha_k^2.
\tag{28}</script><p><code>4.</code> 记$\epsilon_k = f(x_k) - f(x^{*})$，那么：</p>
<script type="math/tex; mode=display">
\min_{k \in \{ 1, \cdots, T \}} \epsilon_k \leq \frac{R^2 + G^2 \sum_{k=1}^{T} \alpha_k^2}{2 \sum_{k=1}^{T} \alpha_k}.
\tag{29}</script><p>上式右边：</p>
<script type="math/tex; mode=display">
\frac{R^2 + G^2 \sum_{k=1}^{T} \alpha_k^2}{2 \sum_{k=1}^{T} \alpha_k} 
\geq \frac{2 RG \sqrt{\sum_{k=1}^{T} \alpha_k^2}}{2 \sum_{k=1}^{T} \alpha_k} 
\geq \frac{RG}{\sqrt{T}} \frac{\sqrt{T} \sqrt{\sum_{k=1}^{T} \alpha_k^2}}{\sum_{k=1}^{T} \alpha_k}.
\tag{30}</script><p>通过选择合适的步长$\alpha_k$，满足$T \geq \sum_{k=1}^{T} \alpha_k^2 \left( \sum_{k=1}^{T} \alpha_k \right)^2$，有：</p>
<script type="math/tex; mode=display">
\min_{k \in \{ 1, \cdots, T \}} \epsilon_k \leq \frac{RG}{\sqrt{T}}.
\tag{31}</script><p>假设$C$是simplex，那么$R \leq \sqrt{2}$。如果每个梯度$g_i$的每个坐标都是以$M$为上界，那么$G$最多可以是$M\sqrt{n}$，即取决于维度。</p>
<p>从步骤2到4，可以用$\text{Div}_{\psi}(x^{<em>},x_{k+1})$代替$| x_{k+1} - x^{</em>} |_2^2$。而步骤1，则需要用到引理1。</p>
<p>假设$\psi$是$\sigma$-严格凸函数，将式(23)中$\alpha_k f(x_k) + \alpha_k \left&lt; g_k, x - x_k \right&gt;$的视为引理1中的$L$，那么：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    & \alpha_k f(x_k) + \alpha_k \left< g_k, x^{*} - x_k \right> + \text{Div}_{\psi}(x^{*}, x_k) \\
    \geq & \alpha_k f(x_k) + \alpha_k \left< g_k, x_{k+1} - x_k \right> + \text{Div}_{\psi}(x_{k+1}, x_{k}) + \text{Div}_{\psi}(x^{*}, x_{k+1}).
\end{aligned}
\tag{32}</script><p>移项后得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{Div}_{\psi}(x^{*}, x_{k+1}) \leq & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left< g_k, x^{*} - x_{k+1} \right> - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\overset{式(4)}{=} & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left< g_k, x^{*} - x_{k} \right> + \alpha_k \left< g_k, x_{k} - x_{k+1} \right> - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*})\right) + \alpha_k \left< g_k, x_{k} - x_{k+1} \right> - \frac{\sigma}{2} \| x_{k} - x_{k+1} \|^2 \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*})\right) + \alpha_k \| g_k \|_{*} \| x_{k} - x_{k+1} \| - \frac{\sigma}{2} \| x_{k} - x_{k+1} \|^2 \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*}) \right) + \frac{ \alpha_k^2 }{2 \sigma} \| g_{k} \|^2.
\end{aligned}
\tag{33}</script><p>与式(26)对比，可以将$| x_{k} - x^{<em>} |_2^2$替换为$\text{Div}_{\psi}(x^{</em>}, x_{k})$。同样，假设$\text{Div}_{\psi}(x^{<em>}, x_{1})$界为$R^2$，且$| g_k |_{</em>}$的界为$G$，其中$| \cdot |_{*}$是对偶范数。</p>
<p>为了显示mirror descent的优势，假设$C$是$n$维的simplex，使用kL散度，其中$\psi$是关于$l_1$范数的1-严格凸函数。那么$l_1$范数的对偶范数就是$l_{\infty}$范数。因此，可以用kL散度考虑$\text{Div}_{\psi}(x^{*}, x_{1})$的界为$\log n$，而$G$则上界为$M$。所以，对于$RG$的值，mirror descent比子梯度下降小一个数量级$O(\sqrt{\frac{n}{\log n}})$。</p>
<p><strong>加速1：$f$是强凸函数。</strong>  我们说关于另一个函数$\psi$和模数$\lambda$，$f$是<code>强凸函数</code>，那么满足：</p>
<script type="math/tex; mode=display">
f(x) \geq f(y) + \left< g, x - y \right> + \lambda \text{Div}_{\psi}(x, y) \quad g \in \partial f(y).
\tag{34}</script><p>注意，并不要求$f$是可微的。那么式(33)可以增加强凸的条件：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{Div}_{\psi}(x^{*}, x_{k+1}) \leq & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left< g_k, x^{*} - x_{k+1} \right> - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\overset{式(4)}{=} & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left< g_k, x^{*} - x_{k} \right> + \alpha_k \left< g_k, x_{k} - x_{k+1} \right> - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*}) + \lambda \text{Div}_{\psi}(x^{*}, x_{k}) \right) + \alpha_k \left< g_k, x_{k} - x_{k+1} \right> - \frac{\sigma}{2} \| x_{k} - x_{k+1} \|^2 \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*}) + \lambda \text{Div}_{\psi}(x^{*}, x_{k}) \right) + \alpha_k \| g_k \|_{*} \| x_{k} - x_{k+1} \| - \frac{\sigma}{2} \| x_{k} - x_{k+1} \|^2 \\
\leq & (1 - \lambda \alpha_k ) \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) - f(x^{*}) \right) + \frac{ \alpha_k^2 }{2 \sigma} \| g_{k} \|_{*}^2.
\end{aligned}
\tag{35}</script><p>记$\delta_{k} = \text{Div}_{\psi}(x^{*}, x_k)$，令$\alpha_k = \frac{1}{\lambda k}$，那么上式有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \delta_{k+1} \leq \frac{k-1}{k} \delta_{k} - \frac{1}{\lambda k} \epsilon_{k} + \frac{G^2}{2 \sigma \lambda^2 k^2} \\
\Rightarrow & k \delta_{k+1} \leq (k-1) \delta_{k} - \frac{1}{\lambda} \epsilon_{k} + \frac{G^2}{2 \sigma \lambda^2 k}.
\end{aligned}
\tag{36}</script><p>递推求和有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& T \delta_{T+1} \leq - \frac{1}{\lambda} \sum_{k=1}^{T} \epsilon_{k} + \frac{G^2}{2 \sigma \lambda^2} \sum_{k=1}^{T} \frac{1}{k}. \\
\Rightarrow & \min_{i \in \{ 1,\cdot, T \}} \epsilon_{k} \leq \frac{G^2}{2 \sigma \lambda } \frac{1}{T} \sum_{k=1}^{T} \frac{1}{k} \leq
\frac{G^2}{2 \sigma \lambda } \frac{O(\log T)}{T}.
\end{aligned}
\tag{37}</script><p><strong>加速2：$f$的梯度Lipschitz连续。</strong> 如果函数$f$的梯度是<code>Lipschitz连续</code>，那么存在$L &gt; 0$使得：</p>
<script type="math/tex; mode=display">
\| \nabla f(x) - \nabla f(y) \|_{*} \leq L \| x - y \|, \quad \forall x, y.
\tag{38}</script><p>有时候我们直接说$f$是光滑的。上式等价于：</p>
<script type="math/tex; mode=display">
f(x) \leq f(y) + \left< \nabla f(y), x - y \right> + \frac{L}{2} \| x - y \|^2.
\tag{39}</script><p>现在考虑式(33)中的$\left&lt; g_k, x^{*} - x_{k+1} \right&gt;$的界：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left< g_k, x^{*} - x_{k+1} \right> =& \left< g_k, x^{*} - x_{k} \right> + \left< g_k, x_{k} - x_{k+1} \right> \\
\leq & f(x^{*}) - f(x_k) +  \frac{L}{2} \| x^{*} - x_k \|^2 + f(x_{k}) - f(x_{k+1}) + \frac{L}{2} \| x_{k} - x_{k+1} \|^2  \\
= & f(x^{*}) - f(x_{k+1}) + \frac{L}{2} \| x_{k} - x_{k+1} \|^2.
\end{aligned}
\tag{40}</script><p>将其代入式(33)有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{Div}_{\psi}(x^{*}, x_{k+1}) \leq & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left( f(x^{*}) - f(x_{k+1}) + \frac{L}{2} \| x_{k} - x_{k+1} \|^2 \right) - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\overset{式(4)}{=}& \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left( f(x^{*}) - f(x_{k+1}) + \frac{L}{2} \| x_{k} - x_{k+1} \|^2 \right) - \frac{\sigma}{2} \| x_{k} - x_{k+1} \|^2.
\end{aligned}
\tag{41}</script><p>令$\alpha_k = \frac{\sigma}{L}$，有：</p>
<script type="math/tex; mode=display">
\text{Div}_{\psi}(x^{*}, x_{k+1}) \leq \text{Div}_{\psi}(x^{*}, x_k) - \frac{\sigma}{L} \left( f(x_{k+1}) - f(x^{*}) \right).
\tag{42}</script><p>递推之：</p>
<script type="math/tex; mode=display">
\min_{k \in \{2,\cdots, T+1\}} f(x_k) - f(x^{*}) \leq \frac{L \text{Div}_{\psi}(x^{*}, x_1)}{\sigma T} \leq \frac{L R^2}{\sigma T}.
\tag{43}</script><p>这时候的收敛速度为$O(\frac{1}{T})$，如果使用像Nesterov的技术，可以达到$O(\frac{1}{T^2})$。可以使用引理1证明，我们称之为<code>加速近似梯度法(accelerated proximal gradient method, APGM)</code>。</p>
<hr>
<h3 id="2-1-组合目标函数"><a href="#2-1-组合目标函数" class="headerlink" title="2.1 组合目标函数"></a>2.1 组合目标函数</h3><p>假设目标函数为$h(x) = f(x) + r(x)$，其中$f$是光滑的，而$r(x)$是simple的，比如$| x |_1$。如果直接使用上面的优化方式，可以得到收敛速度为$O(\frac{1}{\sqrt{T}})$，因为$h$不是光滑的。我们希望能够获得光滑时的收敛速度$O(\frac{1}{T})$是可以办到的，因为$r(x)$是简单的函数，只需要扩展式(23)如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    x_{k + 1} &= argmin_{x \in C} \left\{ f(x_k) + \left< g_k, x - x_k \right> + r(x) + \frac{1}{\alpha_k} \text{Div}_{\psi}(x, x_k) \right\} \\
    &= argmin_{x \in C} \left\{ \alpha_k f(x_k) + \alpha_k \left< g_k, x - x_k \right> + r(x) + \text{Div}_{\psi}(x, x_k)  \right\} \\
    &= argmin_{x \in C} \left\{ \left< \alpha_k g_k, x \right> + r(x) + \text{Div}_{\psi}(x, x_k)  \right\}.
\end{aligned}
\tag{44}</script><p>这里我们只采用了$f$在$x_k$附近的一阶近似，不考虑$r(x)$。假设这个近似操作可以有效地计算，那么我们就可以证明上述的速率都能延续。</p>
<p>这里考虑更为一般的情况，即$f$是非光滑或者强凸的，如果$f$的梯度是Lipschitz连续，依然能够获得$O(\frac{1}{T^2})$的收敛速度。</p>
<p>将$\alpha_k f(x_k) + \alpha_k \left&lt; g_k, x - x_k \right&gt; + r(x)$视作引理1中的$L$，那么有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& \alpha_k f(x_k) + \alpha_k \left< g_k, x^{*} - x_k \right> + r(x^{*}) + \text{Div}_{\psi}(x^{*}, x_k) \\
\geq & \alpha_k f(x_k) + \alpha_k \left< g_k, x_{k+1} - x_k \right> + r(x_{k+1}) + \text{Div}_{\psi}(x_{k+1}, x_k) + \text{Div}_{\psi}(x^{*}, x_{k+1}).
\end{aligned}
\tag{45}</script><p>那么，同式(33),有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{Div}_{\psi}(x^{*}, x_{k+1}) \leq & \text{Div}_{\psi}(x^{*}, x_k) + \alpha_k \left< g_k, x^{*} - x_{k+1} \right> + \alpha_k \left( r(x^{*}) - r(x_{k+1}) \right) - \text{Div}_{\psi}(x_{k+1}, x_{k}) \\
\leq & \cdots \\
\leq & \text{Div}_{\psi}(x^{*}, x_k) - \alpha_k \left( f(x_{k}) + r(x_{k+1}) - f(x^{*}) - f(r(x^{*}) ) \right) + \frac{ \alpha_k^2 }{2 \sigma} \| g_{k} \|^2.
\end{aligned}
\tag{46}</script><p>记$\delta_k = \text{Div}_{\psi}(x^{*}, x_k)$，那么：</p>
<script type="math/tex; mode=display">
f(x_{k}) + r(x_{k+1}) - f(x^{*}) - f(r(x^{*}) \leq \frac{1}{\alpha_k} ( \delta_k - \delta_{k+1} ) + \frac{ \alpha_k }{2 \sigma} \| g_{k} \|_{*}^2.
\tag{47}</script><p>累加求和得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& r(x_{T+1}) - r(x_1) + \sum_{k=1}^{T} \left( h(x_k) - h(x^{*}) \right) \\
\leq & \frac{\delta_1}{\alpha_1} + \sum_{k=2}^{T} \delta_k \left( \frac{1}{\alpha_k} - \frac{1}{\alpha_{k-1}} \right) - \frac{\delta_{T+1}}{\alpha_T} + \frac{ G^2 }{2 \sigma} \sum_{k=1}^{T} \alpha_k \\
\leq & R^2 \left( \frac{1}{\alpha_1} + \sum_{k=2}^{T} \left( \frac{1}{\alpha_k} - \frac{1}{\alpha_{k-1}} \right) \right) + \frac{ G^2 }{2 \sigma} \sum_{k=1}^{T} \alpha_k \\
= & \frac{R^2}{\alpha_T} + \frac{ G^2 }{2 \sigma} \sum_{k=1}^{T} \alpha_k.
\end{aligned}
\tag{48}</script><p>假设选择$x_1 = argmin_{x} r(X)$，那么有$r(x_{T+1}) - r(x_1) \geq 0$。令$\alpha_k = \frac{R}{G} \sqrt{\frac{\sigma}{k}}$，则有：</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{T} \left( h(x_k) - h(x^{*}) \right) \leq \frac{RG}{\sqrt{\sigma}} \left( \sqrt(T) + \frac{1}{2} \sum_{k=1}^{T} \frac{1}{\sqrt{k}} \right) = \frac{RG}{\sqrt{\sigma}} O(\sqrt{T}).
\tag{49}</script><p>因此$\min_{k=1,\cdots,T} \{ h(x_k) - h(x^{*}) \}$的收敛速度为$O(\frac{RG}{\sqrt{\sigma}T})$。</p>
<hr>
<h3 id="2-2-在线学习"><a href="#2-2-在线学习" class="headerlink" title="2.2 在线学习"></a>2.2 在线学习</h3><p><img src="Protocol_of_online_learning_1.jpg" alt=""></p>
<p>上图是在线学习算法。玩家在线学习的目的就是尽量减少遗憾，使得可能的损失$\sum_{k}f_k (x)$在所有可能的$x$上最小：</p>
<script type="math/tex; mode=display">
\text{Regret} = \sum_{k=1}^Tf_k (x_k) - \min_{x} \sum_{k=1}^{T} f_k (x).
\tag{50}</script><p>需要注意的是，没有假设对手如何选$f_k$，可以对立。在第$k$次迭代获得$f_k$后，使用mirror descent对$f_k$进行更新获得$x_{k+1}$：</p>
<script type="math/tex; mode=display">
x_{k+1} = argmin_{x \in C} \left\{ f_k(x_k) + \left< g_k, x - x_k \right> + \frac{1}{\alpha_k} \text{Div}(x, x_k) \right\}, \quad g_k \in \partial f_x (x_k).
\tag{51}</script><p>可以推得regret有界。由式(33)有：</p>
<script type="math/tex; mode=display">
f_k(x_k) - f_k(x^{*}) \leq \frac{1}{\alpha_k} \left( \text{Div}(x^{*}, x_k) - \text{Div}(x^{*}, x_{k+1}) \right) + \frac{\alpha_k}{2 \sigma} \| g_k \|_{*}^{2}.
\tag{52}</script><p>从$k=1$开始累加，同式(48)和式(49)：</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{T} \left( f_k(x_k) - f_k(x^{*}) \right)\leq \frac{RG}{\sqrt{\sigma}} O(\sqrt{T}).
\tag{53}</script><p>所以，regret的是以$O(\sqrt{T})$的速度增长。</p>
<p><strong>$f$是强凸函数</strong> 将$f_k$代替式(35)中的$f$，得到regret的界为$O(\log T)$。</p>
<p><strong>$f$的梯度是Lipschitz连续</strong> 式(43)的结果无法推广到在线学习的情况，如果用$f_k$代替$f$，那么会得到等式右边有$f_k(x_{k+1}) - f_k (x^{*})$。递推求和无法得到regret的界。因此，梯度的Lipschitz连续无法保证regret的界为$O(\log T)$。</p>
<p><strong>组合目标函数</strong> 在在线设定中，玩家和对手都有$r(x)$，对手在每次迭代时改变$f_k(x)$。每次迭代的目标函数是$h_k(x_k) = f_k(x_k) + r(x_k)$。更新规则为：</p>
<script type="math/tex; mode=display">
x_{k+1} = argmin_{x \in C} \left\{ f_k(x_k) + \left< g_k, x - x_k \right>  + r(x) + \frac{1}{\alpha_k} \text{Div}(x, x_k) \right\}, \quad g_k \in \partial f_x (x_k).
\tag{54}</script><p>那么式(47)变为：</p>
<script type="math/tex; mode=display">
f_k(x_{k}) + r(x_{k+1}) - f_k(x^{*}) - f_k(r(x^{*}) \leq \frac{1}{\alpha_k} ( \delta_k - \delta_{k+1} ) + \frac{ \alpha_k }{2 \sigma} \| g_{k} \|_{*}^2.
\tag{55}</script><p>虽然我们这里用的是$r(x_{k+1})$，而不是$r(x_k)$，但这是没有问题的，因为$r$不会通过迭代而改变。<br>选择$x_1 = argmin_{x} r(x)$，同式(48,49)有：</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{T} \left( h_k(x_k) - h_k(x^{*}) \right) \leq \frac{G}{\sqrt{\sigma}} O(\sqrt{T}).
\tag{56}</script><p>因此，有$O(\sqrt{T})$。</p>
<p>当$f_k$为强凸时，我们可以得到组合情况下的$O(\log T)$regret。但是，同上面一样，$f$梯度的Lipschitz连续无法保证regret的界为$O(\log T)$。</p>
<hr>
<h3 id="2-3-随机优化"><a href="#2-3-随机优化" class="headerlink" title="2.3 随机优化"></a>2.3 随机优化</h3><p>让我们考虑优化一个函数，它的期望值形式为</p>
<script type="math/tex; mode=display">
\min_{x} F(x) := \mathbb{E}_{w \sim p} [ f(x; w) ],
\tag{57}</script><p>其中$p$是$w$的分布。这其中包含了很多机器学习模型。比如SVM的目标函数为：</p>
<script type="math/tex; mode=display">
F(x) = \frac{1}{m} \sum_{i=1}^{m} \max \left\{ 0, 1 - c_i \left< a_i, x \right> + \frac{\lambda}{2} \| x \|^2 \right\}.
\tag{58}</script><p>它可以解释成式(57)，$w$是在$\{ 1, 2, \cdots, m \}$上的均匀分布，例如$p(w=i) = \frac{1}{m}$：</p>
<script type="math/tex; mode=display">
f(x; i) = \max \left\{ 0, 1 - c_i \left< a_i, x \right> \right\} + \frac{\lambda}{2} \| x \|^2.
\tag{59}</script><p>当$m$较大时，计算$F$及其子梯度的成本会很高。所以，一个简单的想法是基于一个随机选择的数据点进行更新。它可以认为是算法1中在线学习的一种特殊情况，步骤4中的对手现在随机选取$f_k$为$f(x;w_k)$，$w_k$与$p$无关。理想情况下，我们希望通过使用mirror descent更新，$x_k$将逐渐接近$F(x)$的最小化值。直观上这是非常合理的，通过使用$f_k$，我们可以计算出$F(x_k)$的无偏估计值和$F(x_k)$的次梯度(因为$w_k$是从$p$中进行独立同分布采样得到的)。这是随机优化的一种特殊情况，我们在算法2中进行了总结。</p>
<p><img src="Protocol_of_online_learning_2.jpg" alt=""></p>
<p>事实上，该方法在更一般的环境下也是有效的。为了简单起见，我们只说对手在迭代$k$时有$w_k$。那么在线学习算法$\mathcal{A}$就是简单的从一个有序集$\{ w_1, \cdots, w_k \}$到$x_{k+1}$的确定性映射。将初始模型$x_1$表示为$\mathcal{A}(w_0)$。那么下面的定理就是在线到批量转换的关键。</p>
<p><strong>定理1</strong> 假设在线学习算法$\mathcal{A}$在使用算法1迭代$k$次后regret有上界$R_k$。假设$w_1, \cdots, w_{T+1}$独立同分布采样自$p$。定义$\hat{x}=\mathcal{A}(w_{j+1}, \cdots, w_{T})$，其中$j$均匀随机采样自$\{ 0, \cdots, T \}$。那么：</p>
<script type="math/tex; mode=display">
\mathbb{E}[F(\hat{x})] - \min_{x} F(x) \leq \frac{R_{T+1}}{T + 1}.
\tag{60}</script><p>其中期望值与$w_1, \cdots, w_{T}$和$j$的随机性有关。</p>
<p>相似地，可以用$1 - \sigma$高概率保持边界：</p>
<script type="math/tex; mode=display">
F(\hat{x}) - \min_{x} F(x) \leq \frac{R_{T+1}}{T + 1} \log \frac{1}{\sigma}.
\tag{61}</script><p>其中概率与$w_1, \cdots, w_{T}$和$j$的随机性有关。</p>
<p><em>证明</em> </p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E} [F(\hat{x})] =& \mathbb{E}_{j,w_1, \cdots, w_{T+1}} [f(\hat{x}; w_{T+1})] \\
=& \mathbb{E}_{j,w_1, \cdots, w_{T+1}} [f(\mathcal{A}_{w_{j+1}, \cdots, w_{T}}; w_{T+1})] \\
=& \mathbb{E}_{w_1, \cdots, w_{T+1}} \left[ \frac{1}{T + 1} \sum_{j=0}^{T} f(\mathcal{A}_{w_{j+1}, \cdots, w_{T}}; w_{T+1}) \right] \quad (j \text{是均匀随机抽取的}) \\
=& \frac{1}{T + 1} \mathbb{E}_{w_1, \cdots, w_{T+1}} \left[ \sum_{j=0}^{T} f(\mathcal{A}_{w_{1}, \cdots, w_{T-j}}; w_{T+1-j}) \right] \quad (\text{平移指标}) \\
=& \frac{1}{T + 1} \mathbb{E}_{w_1, \cdots, w_{T+1}} \left[ \sum_{s=1}^{T+1} f(\mathcal{A}_{w_{1}, \cdots, w_{s-1}}; w_{s}) \right] \quad (\text{替换变量} s = T+1-j ) \\
\leq & \frac{1}{T + 1} \mathbb{E}_{w_1, \cdots, w_{T+1}} \left[ \min_{x} \sum_{s=1}^{T+1} f(x; w_{s}) + R_{T+1} \right] \quad (\text{利用regret的界}) \\
\leq & \min_{x} \mathbb{E}_{w} [f(x;w)] + \frac{R_{T+1}}{T+1} \quad (\text{最小值的期望值小于期望值的最小值}) \\ 
= & \min_{x} F(x) + \frac{R_{T+1}}{T + 1}.
\end{aligned}
\tag{62}</script>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" rel="tag"># 优化算法</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/07/11/SOBOLEV%20GRADIENT/" rel="prev" title="Sobolev 梯度">
      <i class="fa fa-chevron-left"></i> Sobolev 梯度
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Bregman-Divergence-and-Mirror-Descent"><span class="nav-number">1.</span> <span class="nav-text">Bregman Divergence and Mirror Descent</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#I-Bregman-Divergence"><span class="nav-number">1.1.</span> <span class="nav-text">I. Bregman Divergence</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#II-Mirror-Descent"><span class="nav-number">1.2.</span> <span class="nav-text">II. Mirror Descent</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-组合目标函数"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 组合目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-在线学习"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 在线学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-随机优化"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 随机优化</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">青山白云间</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">青山白云间</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>
        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
